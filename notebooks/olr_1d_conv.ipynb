{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/apoorva/Desktop/Work/olr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 04:16:30.108217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.utils.load import load_pca_anomaly\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_x, olr_labels = load_pca_anomaly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5920, 5920), (40, 134))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_x.shape, olr_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pentad_data(count):\n",
    "    '''\n",
    "    count is 0-indexed\n",
    "    count = 0 corresponds to first leading pentad\n",
    "    count = 1 corresponds to second leading pentad\n",
    "    count = 2 corresponds to third leading pentad\n",
    "    '''\n",
    "    global olr_labels, pca_x\n",
    "    assert count == 0 or count == 1 or count == 2\n",
    "    pca_x_50 = pca_x[:, :50]\n",
    "    pca_x_50 = np.array([pca_x_50[i*40+j:i*40+j+15, :] for i in range(40) for j in range(134 - (5*count))])\n",
    "    labels = np.reshape(np.reshape(olr_labels, (40, 134))[:, (5*count):], (-1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_x_50, labels, random_state=1337, train_size=0.875, stratify=labels)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d (Separabl  (None, 13, 32)            1782      \n",
      " eConv1D)                                                        \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 13, 32)            128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " separable_conv1d_1 (Separa  (None, 11, 16)            624       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 11, 16)            64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_2 (Separa  (None, 9, 8)              184       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 9, 8)              32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 72)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "235/235 [==============================] - 2s 3ms/step - loss: 0.2236 - f1_score: 0.3189 - categorical_accuracy: 0.6626 - val_loss: 0.1854 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1883 - f1_score: 0.2795 - categorical_accuracy: 0.7220 - val_loss: 0.1844 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1831 - f1_score: 0.2809 - categorical_accuracy: 0.7220 - val_loss: 0.1856 - val_f1_score: 0.2816 - val_categorical_accuracy: 0.7313\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1812 - f1_score: 0.2864 - categorical_accuracy: 0.7236 - val_loss: 0.1852 - val_f1_score: 0.3071 - val_categorical_accuracy: 0.7313\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1777 - f1_score: 0.3087 - categorical_accuracy: 0.7223 - val_loss: 0.1843 - val_f1_score: 0.3116 - val_categorical_accuracy: 0.7292\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1752 - f1_score: 0.3149 - categorical_accuracy: 0.7231 - val_loss: 0.1863 - val_f1_score: 0.3399 - val_categorical_accuracy: 0.7271\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1724 - f1_score: 0.3497 - categorical_accuracy: 0.7271 - val_loss: 0.1865 - val_f1_score: 0.3572 - val_categorical_accuracy: 0.7271\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1698 - f1_score: 0.3480 - categorical_accuracy: 0.7255 - val_loss: 0.1884 - val_f1_score: 0.3406 - val_categorical_accuracy: 0.7249\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1682 - f1_score: 0.3699 - categorical_accuracy: 0.7255 - val_loss: 0.1840 - val_f1_score: 0.3618 - val_categorical_accuracy: 0.7249\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1647 - f1_score: 0.3778 - categorical_accuracy: 0.7260 - val_loss: 0.1894 - val_f1_score: 0.3810 - val_categorical_accuracy: 0.7079\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1632 - f1_score: 0.4041 - categorical_accuracy: 0.7281 - val_loss: 0.1884 - val_f1_score: 0.3465 - val_categorical_accuracy: 0.7196\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1601 - f1_score: 0.3962 - categorical_accuracy: 0.7292 - val_loss: 0.1894 - val_f1_score: 0.3603 - val_categorical_accuracy: 0.6951\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1582 - f1_score: 0.4049 - categorical_accuracy: 0.7263 - val_loss: 0.1925 - val_f1_score: 0.3605 - val_categorical_accuracy: 0.6930\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1561 - f1_score: 0.4365 - categorical_accuracy: 0.7388 - val_loss: 0.1883 - val_f1_score: 0.3665 - val_categorical_accuracy: 0.7132\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1561 - f1_score: 0.4291 - categorical_accuracy: 0.7321 - val_loss: 0.1924 - val_f1_score: 0.3903 - val_categorical_accuracy: 0.6983\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1533 - f1_score: 0.4567 - categorical_accuracy: 0.7401 - val_loss: 0.1944 - val_f1_score: 0.3747 - val_categorical_accuracy: 0.6940\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1509 - f1_score: 0.4458 - categorical_accuracy: 0.7356 - val_loss: 0.1949 - val_f1_score: 0.3755 - val_categorical_accuracy: 0.6866\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1502 - f1_score: 0.4536 - categorical_accuracy: 0.7364 - val_loss: 0.1963 - val_f1_score: 0.3613 - val_categorical_accuracy: 0.6962\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1467 - f1_score: 0.4648 - categorical_accuracy: 0.7364 - val_loss: 0.1990 - val_f1_score: 0.3780 - val_categorical_accuracy: 0.6844\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1466 - f1_score: 0.4572 - categorical_accuracy: 0.7353 - val_loss: 0.1991 - val_f1_score: 0.3968 - val_categorical_accuracy: 0.6802\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1451 - f1_score: 0.4697 - categorical_accuracy: 0.7396 - val_loss: 0.2015 - val_f1_score: 0.3792 - val_categorical_accuracy: 0.6759\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1438 - f1_score: 0.4717 - categorical_accuracy: 0.7353 - val_loss: 0.1979 - val_f1_score: 0.3859 - val_categorical_accuracy: 0.6738\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1430 - f1_score: 0.4882 - categorical_accuracy: 0.7425 - val_loss: 0.2012 - val_f1_score: 0.3780 - val_categorical_accuracy: 0.6748\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1408 - f1_score: 0.5126 - categorical_accuracy: 0.7463 - val_loss: 0.1990 - val_f1_score: 0.3762 - val_categorical_accuracy: 0.7004\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1403 - f1_score: 0.4858 - categorical_accuracy: 0.7375 - val_loss: 0.2013 - val_f1_score: 0.3723 - val_categorical_accuracy: 0.6791\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1392 - f1_score: 0.4917 - categorical_accuracy: 0.7415 - val_loss: 0.2005 - val_f1_score: 0.3651 - val_categorical_accuracy: 0.6620\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1372 - f1_score: 0.4967 - categorical_accuracy: 0.7439 - val_loss: 0.1989 - val_f1_score: 0.3720 - val_categorical_accuracy: 0.6716\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1328 - f1_score: 0.5127 - categorical_accuracy: 0.7433 - val_loss: 0.2075 - val_f1_score: 0.3820 - val_categorical_accuracy: 0.6642\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1360 - f1_score: 0.5127 - categorical_accuracy: 0.7449 - val_loss: 0.2050 - val_f1_score: 0.4009 - val_categorical_accuracy: 0.6674\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1341 - f1_score: 0.5069 - categorical_accuracy: 0.7423 - val_loss: 0.2046 - val_f1_score: 0.3902 - val_categorical_accuracy: 0.6738\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1328 - f1_score: 0.5092 - categorical_accuracy: 0.7431 - val_loss: 0.2078 - val_f1_score: 0.3794 - val_categorical_accuracy: 0.6652\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1318 - f1_score: 0.5255 - categorical_accuracy: 0.7473 - val_loss: 0.2098 - val_f1_score: 0.3837 - val_categorical_accuracy: 0.6610\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1316 - f1_score: 0.5256 - categorical_accuracy: 0.7492 - val_loss: 0.2159 - val_f1_score: 0.3742 - val_categorical_accuracy: 0.6354\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1315 - f1_score: 0.5255 - categorical_accuracy: 0.7428 - val_loss: 0.2157 - val_f1_score: 0.3798 - val_categorical_accuracy: 0.6642\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1298 - f1_score: 0.5249 - categorical_accuracy: 0.7468 - val_loss: 0.2151 - val_f1_score: 0.3802 - val_categorical_accuracy: 0.6567\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1296 - f1_score: 0.5266 - categorical_accuracy: 0.7423 - val_loss: 0.2137 - val_f1_score: 0.3623 - val_categorical_accuracy: 0.6770\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1275 - f1_score: 0.5359 - categorical_accuracy: 0.7503 - val_loss: 0.2174 - val_f1_score: 0.3800 - val_categorical_accuracy: 0.6652\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1271 - f1_score: 0.5330 - categorical_accuracy: 0.7484 - val_loss: 0.2203 - val_f1_score: 0.3868 - val_categorical_accuracy: 0.6279\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1272 - f1_score: 0.5412 - categorical_accuracy: 0.7481 - val_loss: 0.2164 - val_f1_score: 0.3651 - val_categorical_accuracy: 0.6439\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1272 - f1_score: 0.5232 - categorical_accuracy: 0.7404 - val_loss: 0.2206 - val_f1_score: 0.3692 - val_categorical_accuracy: 0.6450\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1256 - f1_score: 0.5196 - categorical_accuracy: 0.7359 - val_loss: 0.2181 - val_f1_score: 0.3751 - val_categorical_accuracy: 0.6514\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1261 - f1_score: 0.5244 - categorical_accuracy: 0.7407 - val_loss: 0.2182 - val_f1_score: 0.3728 - val_categorical_accuracy: 0.6748\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1252 - f1_score: 0.5394 - categorical_accuracy: 0.7492 - val_loss: 0.2214 - val_f1_score: 0.3653 - val_categorical_accuracy: 0.6493\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1229 - f1_score: 0.5450 - categorical_accuracy: 0.7545 - val_loss: 0.2226 - val_f1_score: 0.3814 - val_categorical_accuracy: 0.6375\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1241 - f1_score: 0.5465 - categorical_accuracy: 0.7505 - val_loss: 0.2199 - val_f1_score: 0.3819 - val_categorical_accuracy: 0.6546\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1241 - f1_score: 0.5473 - categorical_accuracy: 0.7524 - val_loss: 0.2252 - val_f1_score: 0.3787 - val_categorical_accuracy: 0.6290\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1236 - f1_score: 0.5506 - categorical_accuracy: 0.7492 - val_loss: 0.2249 - val_f1_score: 0.3725 - val_categorical_accuracy: 0.6354\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1220 - f1_score: 0.5540 - categorical_accuracy: 0.7516 - val_loss: 0.2243 - val_f1_score: 0.3657 - val_categorical_accuracy: 0.6599\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1220 - f1_score: 0.5428 - categorical_accuracy: 0.7489 - val_loss: 0.2225 - val_f1_score: 0.3693 - val_categorical_accuracy: 0.6503\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1205 - f1_score: 0.5536 - categorical_accuracy: 0.7511 - val_loss: 0.2274 - val_f1_score: 0.3805 - val_categorical_accuracy: 0.6557\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1225 - f1_score: 0.5484 - categorical_accuracy: 0.7497 - val_loss: 0.2247 - val_f1_score: 0.3790 - val_categorical_accuracy: 0.6471\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1207 - f1_score: 0.5515 - categorical_accuracy: 0.7487 - val_loss: 0.2252 - val_f1_score: 0.3818 - val_categorical_accuracy: 0.6503\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1196 - f1_score: 0.5571 - categorical_accuracy: 0.7553 - val_loss: 0.2266 - val_f1_score: 0.3857 - val_categorical_accuracy: 0.6450\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1204 - f1_score: 0.5418 - categorical_accuracy: 0.7460 - val_loss: 0.2291 - val_f1_score: 0.3535 - val_categorical_accuracy: 0.6375\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1187 - f1_score: 0.5571 - categorical_accuracy: 0.7481 - val_loss: 0.2308 - val_f1_score: 0.3705 - val_categorical_accuracy: 0.6429\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1181 - f1_score: 0.5513 - categorical_accuracy: 0.7484 - val_loss: 0.2328 - val_f1_score: 0.3839 - val_categorical_accuracy: 0.6567\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1203 - f1_score: 0.5502 - categorical_accuracy: 0.7487 - val_loss: 0.2317 - val_f1_score: 0.3823 - val_categorical_accuracy: 0.6514\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1168 - f1_score: 0.5462 - categorical_accuracy: 0.7487 - val_loss: 0.2333 - val_f1_score: 0.3838 - val_categorical_accuracy: 0.6461\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1176 - f1_score: 0.5614 - categorical_accuracy: 0.7508 - val_loss: 0.2335 - val_f1_score: 0.3579 - val_categorical_accuracy: 0.6354\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1168 - f1_score: 0.5568 - categorical_accuracy: 0.7503 - val_loss: 0.2340 - val_f1_score: 0.3712 - val_categorical_accuracy: 0.6439\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1165 - f1_score: 0.5510 - categorical_accuracy: 0.7489 - val_loss: 0.2356 - val_f1_score: 0.3801 - val_categorical_accuracy: 0.6194\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1171 - f1_score: 0.5495 - categorical_accuracy: 0.7479 - val_loss: 0.2377 - val_f1_score: 0.3625 - val_categorical_accuracy: 0.6119\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1173 - f1_score: 0.5703 - categorical_accuracy: 0.7572 - val_loss: 0.2346 - val_f1_score: 0.3606 - val_categorical_accuracy: 0.6333\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1163 - f1_score: 0.5459 - categorical_accuracy: 0.7484 - val_loss: 0.2391 - val_f1_score: 0.3722 - val_categorical_accuracy: 0.6258\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1145 - f1_score: 0.5623 - categorical_accuracy: 0.7500 - val_loss: 0.2346 - val_f1_score: 0.3711 - val_categorical_accuracy: 0.6471\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1145 - f1_score: 0.5602 - categorical_accuracy: 0.7516 - val_loss: 0.2411 - val_f1_score: 0.3754 - val_categorical_accuracy: 0.6205\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1169 - f1_score: 0.5560 - categorical_accuracy: 0.7449 - val_loss: 0.2347 - val_f1_score: 0.3589 - val_categorical_accuracy: 0.6226\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1153 - f1_score: 0.5563 - categorical_accuracy: 0.7487 - val_loss: 0.2421 - val_f1_score: 0.3569 - val_categorical_accuracy: 0.6247\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1132 - f1_score: 0.5706 - categorical_accuracy: 0.7511 - val_loss: 0.2426 - val_f1_score: 0.3689 - val_categorical_accuracy: 0.6279\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1150 - f1_score: 0.5523 - categorical_accuracy: 0.7479 - val_loss: 0.2413 - val_f1_score: 0.3705 - val_categorical_accuracy: 0.6194\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1130 - f1_score: 0.5633 - categorical_accuracy: 0.7511 - val_loss: 0.2511 - val_f1_score: 0.3515 - val_categorical_accuracy: 0.6173\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1141 - f1_score: 0.5668 - categorical_accuracy: 0.7500 - val_loss: 0.2418 - val_f1_score: 0.3817 - val_categorical_accuracy: 0.6375\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1131 - f1_score: 0.5671 - categorical_accuracy: 0.7540 - val_loss: 0.2472 - val_f1_score: 0.3798 - val_categorical_accuracy: 0.6301\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1133 - f1_score: 0.5723 - categorical_accuracy: 0.7537 - val_loss: 0.2418 - val_f1_score: 0.3675 - val_categorical_accuracy: 0.6493\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1145 - f1_score: 0.5514 - categorical_accuracy: 0.7428 - val_loss: 0.2360 - val_f1_score: 0.3592 - val_categorical_accuracy: 0.6354\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1134 - f1_score: 0.5664 - categorical_accuracy: 0.7540 - val_loss: 0.2478 - val_f1_score: 0.3514 - val_categorical_accuracy: 0.6151\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1137 - f1_score: 0.5765 - categorical_accuracy: 0.7559 - val_loss: 0.2451 - val_f1_score: 0.3614 - val_categorical_accuracy: 0.6130\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1116 - f1_score: 0.5730 - categorical_accuracy: 0.7551 - val_loss: 0.2524 - val_f1_score: 0.3605 - val_categorical_accuracy: 0.6023\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1112 - f1_score: 0.5817 - categorical_accuracy: 0.7559 - val_loss: 0.2467 - val_f1_score: 0.3844 - val_categorical_accuracy: 0.6525\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1109 - f1_score: 0.5674 - categorical_accuracy: 0.7524 - val_loss: 0.2532 - val_f1_score: 0.3893 - val_categorical_accuracy: 0.6130\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1137 - f1_score: 0.5595 - categorical_accuracy: 0.7463 - val_loss: 0.2446 - val_f1_score: 0.3602 - val_categorical_accuracy: 0.6173\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1115 - f1_score: 0.5823 - categorical_accuracy: 0.7593 - val_loss: 0.2482 - val_f1_score: 0.3538 - val_categorical_accuracy: 0.6002\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1122 - f1_score: 0.5786 - categorical_accuracy: 0.7583 - val_loss: 0.2481 - val_f1_score: 0.3652 - val_categorical_accuracy: 0.6087\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1104 - f1_score: 0.5659 - categorical_accuracy: 0.7556 - val_loss: 0.2506 - val_f1_score: 0.3608 - val_categorical_accuracy: 0.6087\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1108 - f1_score: 0.5779 - categorical_accuracy: 0.7561 - val_loss: 0.2473 - val_f1_score: 0.3719 - val_categorical_accuracy: 0.6471\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1113 - f1_score: 0.5680 - categorical_accuracy: 0.7500 - val_loss: 0.2486 - val_f1_score: 0.3766 - val_categorical_accuracy: 0.6418\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1107 - f1_score: 0.5708 - categorical_accuracy: 0.7545 - val_loss: 0.2466 - val_f1_score: 0.3587 - val_categorical_accuracy: 0.6322\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1104 - f1_score: 0.5749 - categorical_accuracy: 0.7561 - val_loss: 0.2471 - val_f1_score: 0.3656 - val_categorical_accuracy: 0.6247\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1096 - f1_score: 0.5733 - categorical_accuracy: 0.7516 - val_loss: 0.2552 - val_f1_score: 0.3763 - val_categorical_accuracy: 0.6397\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1101 - f1_score: 0.5865 - categorical_accuracy: 0.7609 - val_loss: 0.2526 - val_f1_score: 0.3463 - val_categorical_accuracy: 0.5853\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1094 - f1_score: 0.5788 - categorical_accuracy: 0.7556 - val_loss: 0.2517 - val_f1_score: 0.3712 - val_categorical_accuracy: 0.6450\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1101 - f1_score: 0.5724 - categorical_accuracy: 0.7537 - val_loss: 0.2544 - val_f1_score: 0.3570 - val_categorical_accuracy: 0.6034\n",
      "Epoch 93/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1099 - f1_score: 0.5802 - categorical_accuracy: 0.7564 - val_loss: 0.2515 - val_f1_score: 0.3544 - val_categorical_accuracy: 0.6162\n",
      "Epoch 94/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1099 - f1_score: 0.5771 - categorical_accuracy: 0.7548 - val_loss: 0.2567 - val_f1_score: 0.3431 - val_categorical_accuracy: 0.6173\n",
      "Epoch 95/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1085 - f1_score: 0.5748 - categorical_accuracy: 0.7516 - val_loss: 0.2614 - val_f1_score: 0.3638 - val_categorical_accuracy: 0.6151\n",
      "Epoch 96/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1091 - f1_score: 0.5748 - categorical_accuracy: 0.7553 - val_loss: 0.2578 - val_f1_score: 0.3543 - val_categorical_accuracy: 0.6098\n",
      "Epoch 97/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1091 - f1_score: 0.5760 - categorical_accuracy: 0.7564 - val_loss: 0.2510 - val_f1_score: 0.3505 - val_categorical_accuracy: 0.6066\n",
      "Epoch 98/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1104 - f1_score: 0.5680 - categorical_accuracy: 0.7540 - val_loss: 0.2628 - val_f1_score: 0.3533 - val_categorical_accuracy: 0.5981\n",
      "Epoch 99/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1089 - f1_score: 0.6007 - categorical_accuracy: 0.7652 - val_loss: 0.2636 - val_f1_score: 0.3528 - val_categorical_accuracy: 0.5949\n",
      "Epoch 100/100\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.1078 - f1_score: 0.5978 - categorical_accuracy: 0.7633 - val_loss: 0.2631 - val_f1_score: 0.3574 - val_categorical_accuracy: 0.5853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x779987eee110>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.19e-3\n",
    "EPOCHS=100\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.3, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 0s 807us/step\n",
      "Training\n",
      "[[ 272  306   36]\n",
      " [ 215 2876  310]\n",
      " [  65  306  304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.44      0.47       614\n",
      "         1.0       0.82      0.85      0.83      3401\n",
      "         2.0       0.47      0.45      0.46       675\n",
      "\n",
      "    accuracy                           0.74      4690\n",
      "   macro avg       0.59      0.58      0.59      4690\n",
      "weighted avg       0.73      0.74      0.73      4690\n",
      "\n",
      "21/21 [==============================] - 0s 829us/step\n",
      "Testing\n",
      "[[ 16  64   8]\n",
      " [ 53 369  64]\n",
      " [ 12  66  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.18      0.19        88\n",
      "         1.0       0.74      0.76      0.75       486\n",
      "         2.0       0.20      0.19      0.19        96\n",
      "\n",
      "    accuracy                           0.60       670\n",
      "   macro avg       0.38      0.38      0.38       670\n",
      "weighted avg       0.59      0.60      0.60       670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(1)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d_3 (Separa  (None, 13, 32)            1782      \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 13, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_4 (Separa  (None, 11, 16)            624       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 11, 16)            64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_5 (Separa  (None, 9, 8)              184       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 9, 8)              32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "226/226 [==============================] - 2s 3ms/step - loss: 0.1238 - f1_score: 0.3225 - categorical_accuracy: 0.6096 - val_loss: 0.0979 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 2/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0982 - f1_score: 0.2814 - categorical_accuracy: 0.7146 - val_loss: 0.0967 - val_f1_score: 0.2939 - val_categorical_accuracy: 0.7364\n",
      "Epoch 3/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0955 - f1_score: 0.2841 - categorical_accuracy: 0.7154 - val_loss: 0.0934 - val_f1_score: 0.2879 - val_categorical_accuracy: 0.7342\n",
      "Epoch 4/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0936 - f1_score: 0.2885 - categorical_accuracy: 0.7151 - val_loss: 0.0920 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 5/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0922 - f1_score: 0.3015 - categorical_accuracy: 0.7168 - val_loss: 0.0917 - val_f1_score: 0.3243 - val_categorical_accuracy: 0.7231\n",
      "Epoch 6/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0908 - f1_score: 0.3104 - categorical_accuracy: 0.7159 - val_loss: 0.0904 - val_f1_score: 0.2977 - val_categorical_accuracy: 0.7320\n",
      "Epoch 7/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0892 - f1_score: 0.3311 - categorical_accuracy: 0.7195 - val_loss: 0.0904 - val_f1_score: 0.3234 - val_categorical_accuracy: 0.7353\n",
      "Epoch 8/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0878 - f1_score: 0.3434 - categorical_accuracy: 0.7215 - val_loss: 0.0900 - val_f1_score: 0.3378 - val_categorical_accuracy: 0.7298\n",
      "Epoch 9/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0868 - f1_score: 0.3660 - categorical_accuracy: 0.7243 - val_loss: 0.0887 - val_f1_score: 0.3298 - val_categorical_accuracy: 0.7386\n",
      "Epoch 10/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0856 - f1_score: 0.3655 - categorical_accuracy: 0.7212 - val_loss: 0.0887 - val_f1_score: 0.3887 - val_categorical_accuracy: 0.7331\n",
      "Epoch 11/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0842 - f1_score: 0.3863 - categorical_accuracy: 0.7229 - val_loss: 0.0864 - val_f1_score: 0.3723 - val_categorical_accuracy: 0.7353\n",
      "Epoch 12/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0834 - f1_score: 0.4068 - categorical_accuracy: 0.7279 - val_loss: 0.0870 - val_f1_score: 0.4111 - val_categorical_accuracy: 0.7254\n",
      "Epoch 13/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0825 - f1_score: 0.4145 - categorical_accuracy: 0.7273 - val_loss: 0.0873 - val_f1_score: 0.3924 - val_categorical_accuracy: 0.7243\n",
      "Epoch 14/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0806 - f1_score: 0.4247 - categorical_accuracy: 0.7284 - val_loss: 0.0849 - val_f1_score: 0.3963 - val_categorical_accuracy: 0.7243\n",
      "Epoch 15/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0806 - f1_score: 0.4274 - categorical_accuracy: 0.7295 - val_loss: 0.0855 - val_f1_score: 0.3818 - val_categorical_accuracy: 0.7254\n",
      "Epoch 16/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0812 - f1_score: 0.4293 - categorical_accuracy: 0.7315 - val_loss: 0.0864 - val_f1_score: 0.4393 - val_categorical_accuracy: 0.7176\n",
      "Epoch 17/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0798 - f1_score: 0.4464 - categorical_accuracy: 0.7342 - val_loss: 0.0862 - val_f1_score: 0.4107 - val_categorical_accuracy: 0.7287\n",
      "Epoch 18/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0784 - f1_score: 0.4488 - categorical_accuracy: 0.7342 - val_loss: 0.0855 - val_f1_score: 0.4223 - val_categorical_accuracy: 0.7276\n",
      "Epoch 19/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0785 - f1_score: 0.4456 - categorical_accuracy: 0.7326 - val_loss: 0.0842 - val_f1_score: 0.4186 - val_categorical_accuracy: 0.7220\n",
      "Epoch 20/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0777 - f1_score: 0.4594 - categorical_accuracy: 0.7337 - val_loss: 0.0857 - val_f1_score: 0.4118 - val_categorical_accuracy: 0.7265\n",
      "Epoch 21/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0765 - f1_score: 0.4695 - categorical_accuracy: 0.7348 - val_loss: 0.0860 - val_f1_score: 0.4422 - val_categorical_accuracy: 0.7165\n",
      "Epoch 22/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0760 - f1_score: 0.4707 - categorical_accuracy: 0.7350 - val_loss: 0.0855 - val_f1_score: 0.4236 - val_categorical_accuracy: 0.7087\n",
      "Epoch 23/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0754 - f1_score: 0.4961 - categorical_accuracy: 0.7370 - val_loss: 0.0850 - val_f1_score: 0.4263 - val_categorical_accuracy: 0.7320\n",
      "Epoch 24/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0754 - f1_score: 0.4726 - categorical_accuracy: 0.7362 - val_loss: 0.0846 - val_f1_score: 0.4321 - val_categorical_accuracy: 0.7298\n",
      "Epoch 25/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0751 - f1_score: 0.4731 - categorical_accuracy: 0.7301 - val_loss: 0.0855 - val_f1_score: 0.4567 - val_categorical_accuracy: 0.7287\n",
      "Epoch 26/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0744 - f1_score: 0.4836 - categorical_accuracy: 0.7320 - val_loss: 0.0856 - val_f1_score: 0.4371 - val_categorical_accuracy: 0.7110\n",
      "Epoch 27/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0743 - f1_score: 0.4943 - categorical_accuracy: 0.7389 - val_loss: 0.0840 - val_f1_score: 0.4288 - val_categorical_accuracy: 0.7110\n",
      "Epoch 28/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0738 - f1_score: 0.4900 - categorical_accuracy: 0.7359 - val_loss: 0.0857 - val_f1_score: 0.4335 - val_categorical_accuracy: 0.7187\n",
      "Epoch 29/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0735 - f1_score: 0.4910 - categorical_accuracy: 0.7356 - val_loss: 0.0835 - val_f1_score: 0.4500 - val_categorical_accuracy: 0.7154\n",
      "Epoch 30/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0725 - f1_score: 0.5086 - categorical_accuracy: 0.7400 - val_loss: 0.0850 - val_f1_score: 0.4355 - val_categorical_accuracy: 0.7099\n",
      "Epoch 31/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0724 - f1_score: 0.5026 - categorical_accuracy: 0.7392 - val_loss: 0.0858 - val_f1_score: 0.4397 - val_categorical_accuracy: 0.7132\n",
      "Epoch 32/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0716 - f1_score: 0.5216 - categorical_accuracy: 0.7422 - val_loss: 0.0837 - val_f1_score: 0.4428 - val_categorical_accuracy: 0.7132\n",
      "Epoch 33/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0715 - f1_score: 0.5111 - categorical_accuracy: 0.7425 - val_loss: 0.0847 - val_f1_score: 0.4958 - val_categorical_accuracy: 0.7187\n",
      "Epoch 34/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0718 - f1_score: 0.5110 - categorical_accuracy: 0.7381 - val_loss: 0.0843 - val_f1_score: 0.4252 - val_categorical_accuracy: 0.7043\n",
      "Epoch 35/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0705 - f1_score: 0.5070 - categorical_accuracy: 0.7370 - val_loss: 0.0831 - val_f1_score: 0.4622 - val_categorical_accuracy: 0.7220\n",
      "Epoch 36/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0712 - f1_score: 0.5020 - categorical_accuracy: 0.7339 - val_loss: 0.0852 - val_f1_score: 0.4338 - val_categorical_accuracy: 0.6955\n",
      "Epoch 37/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0702 - f1_score: 0.5161 - categorical_accuracy: 0.7422 - val_loss: 0.0872 - val_f1_score: 0.4625 - val_categorical_accuracy: 0.7132\n",
      "Epoch 38/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0703 - f1_score: 0.5108 - categorical_accuracy: 0.7384 - val_loss: 0.0877 - val_f1_score: 0.4377 - val_categorical_accuracy: 0.7110\n",
      "Epoch 39/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0704 - f1_score: 0.5234 - categorical_accuracy: 0.7425 - val_loss: 0.0877 - val_f1_score: 0.4615 - val_categorical_accuracy: 0.6977\n",
      "Epoch 40/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0697 - f1_score: 0.5124 - categorical_accuracy: 0.7348 - val_loss: 0.0851 - val_f1_score: 0.4505 - val_categorical_accuracy: 0.7198\n",
      "Epoch 41/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0694 - f1_score: 0.5333 - categorical_accuracy: 0.7425 - val_loss: 0.0870 - val_f1_score: 0.4172 - val_categorical_accuracy: 0.6955\n",
      "Epoch 42/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0695 - f1_score: 0.5135 - categorical_accuracy: 0.7334 - val_loss: 0.0836 - val_f1_score: 0.4723 - val_categorical_accuracy: 0.7243\n",
      "Epoch 43/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0687 - f1_score: 0.5261 - categorical_accuracy: 0.7436 - val_loss: 0.0851 - val_f1_score: 0.4639 - val_categorical_accuracy: 0.6977\n",
      "Epoch 44/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0684 - f1_score: 0.5156 - categorical_accuracy: 0.7348 - val_loss: 0.0867 - val_f1_score: 0.4286 - val_categorical_accuracy: 0.7054\n",
      "Epoch 45/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0690 - f1_score: 0.5214 - categorical_accuracy: 0.7400 - val_loss: 0.0861 - val_f1_score: 0.4221 - val_categorical_accuracy: 0.7076\n",
      "Epoch 46/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0673 - f1_score: 0.5215 - categorical_accuracy: 0.7373 - val_loss: 0.0866 - val_f1_score: 0.4656 - val_categorical_accuracy: 0.7154\n",
      "Epoch 47/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0673 - f1_score: 0.5374 - categorical_accuracy: 0.7414 - val_loss: 0.0893 - val_f1_score: 0.4166 - val_categorical_accuracy: 0.6988\n",
      "Epoch 48/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0678 - f1_score: 0.5342 - categorical_accuracy: 0.7458 - val_loss: 0.0883 - val_f1_score: 0.4354 - val_categorical_accuracy: 0.6999\n",
      "Epoch 49/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0678 - f1_score: 0.5425 - categorical_accuracy: 0.7445 - val_loss: 0.0868 - val_f1_score: 0.4199 - val_categorical_accuracy: 0.6932\n",
      "Epoch 50/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0674 - f1_score: 0.5339 - categorical_accuracy: 0.7461 - val_loss: 0.0866 - val_f1_score: 0.4355 - val_categorical_accuracy: 0.7065\n",
      "Epoch 51/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0672 - f1_score: 0.5336 - categorical_accuracy: 0.7472 - val_loss: 0.0863 - val_f1_score: 0.4459 - val_categorical_accuracy: 0.6899\n",
      "Epoch 52/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0665 - f1_score: 0.5316 - categorical_accuracy: 0.7403 - val_loss: 0.0882 - val_f1_score: 0.4377 - val_categorical_accuracy: 0.7076\n",
      "Epoch 53/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0669 - f1_score: 0.5420 - categorical_accuracy: 0.7467 - val_loss: 0.0894 - val_f1_score: 0.4696 - val_categorical_accuracy: 0.7054\n",
      "Epoch 54/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0668 - f1_score: 0.5428 - categorical_accuracy: 0.7453 - val_loss: 0.0880 - val_f1_score: 0.4486 - val_categorical_accuracy: 0.7076\n",
      "Epoch 55/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0658 - f1_score: 0.5366 - categorical_accuracy: 0.7483 - val_loss: 0.0880 - val_f1_score: 0.4388 - val_categorical_accuracy: 0.6811\n",
      "Epoch 56/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0664 - f1_score: 0.5385 - categorical_accuracy: 0.7392 - val_loss: 0.0892 - val_f1_score: 0.4623 - val_categorical_accuracy: 0.7110\n",
      "Epoch 57/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0661 - f1_score: 0.5476 - categorical_accuracy: 0.7467 - val_loss: 0.0905 - val_f1_score: 0.4468 - val_categorical_accuracy: 0.6977\n",
      "Epoch 58/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0660 - f1_score: 0.5293 - categorical_accuracy: 0.7428 - val_loss: 0.0892 - val_f1_score: 0.4429 - val_categorical_accuracy: 0.6888\n",
      "Epoch 59/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0661 - f1_score: 0.5423 - categorical_accuracy: 0.7434 - val_loss: 0.0891 - val_f1_score: 0.4418 - val_categorical_accuracy: 0.6944\n",
      "Epoch 60/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0647 - f1_score: 0.5493 - categorical_accuracy: 0.7472 - val_loss: 0.0921 - val_f1_score: 0.4401 - val_categorical_accuracy: 0.7032\n",
      "Epoch 61/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0653 - f1_score: 0.5417 - categorical_accuracy: 0.7450 - val_loss: 0.0900 - val_f1_score: 0.4412 - val_categorical_accuracy: 0.6866\n",
      "Epoch 62/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0652 - f1_score: 0.5555 - categorical_accuracy: 0.7500 - val_loss: 0.0908 - val_f1_score: 0.4365 - val_categorical_accuracy: 0.6877\n",
      "Epoch 63/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0650 - f1_score: 0.5404 - categorical_accuracy: 0.7409 - val_loss: 0.0904 - val_f1_score: 0.4485 - val_categorical_accuracy: 0.6932\n",
      "Epoch 64/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0650 - f1_score: 0.5615 - categorical_accuracy: 0.7506 - val_loss: 0.0920 - val_f1_score: 0.4339 - val_categorical_accuracy: 0.6833\n",
      "Epoch 65/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0649 - f1_score: 0.5433 - categorical_accuracy: 0.7425 - val_loss: 0.0917 - val_f1_score: 0.4530 - val_categorical_accuracy: 0.7043\n",
      "Epoch 66/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0644 - f1_score: 0.5468 - categorical_accuracy: 0.7475 - val_loss: 0.0920 - val_f1_score: 0.4408 - val_categorical_accuracy: 0.6777\n",
      "Epoch 67/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0641 - f1_score: 0.5430 - categorical_accuracy: 0.7434 - val_loss: 0.0916 - val_f1_score: 0.4282 - val_categorical_accuracy: 0.6700\n",
      "Epoch 68/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0644 - f1_score: 0.5592 - categorical_accuracy: 0.7497 - val_loss: 0.0922 - val_f1_score: 0.4408 - val_categorical_accuracy: 0.6910\n",
      "Epoch 69/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0649 - f1_score: 0.5403 - categorical_accuracy: 0.7409 - val_loss: 0.0907 - val_f1_score: 0.4327 - val_categorical_accuracy: 0.6877\n",
      "Epoch 70/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0645 - f1_score: 0.5460 - categorical_accuracy: 0.7425 - val_loss: 0.0917 - val_f1_score: 0.4475 - val_categorical_accuracy: 0.7010\n",
      "Epoch 71/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0646 - f1_score: 0.5412 - categorical_accuracy: 0.7422 - val_loss: 0.0932 - val_f1_score: 0.4568 - val_categorical_accuracy: 0.7043\n",
      "Epoch 72/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0640 - f1_score: 0.5487 - categorical_accuracy: 0.7456 - val_loss: 0.0921 - val_f1_score: 0.4454 - val_categorical_accuracy: 0.6910\n",
      "Epoch 73/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0632 - f1_score: 0.5561 - categorical_accuracy: 0.7494 - val_loss: 0.0924 - val_f1_score: 0.4316 - val_categorical_accuracy: 0.6800\n",
      "Epoch 74/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0634 - f1_score: 0.5521 - categorical_accuracy: 0.7483 - val_loss: 0.0950 - val_f1_score: 0.4293 - val_categorical_accuracy: 0.6755\n",
      "Epoch 75/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0639 - f1_score: 0.5493 - categorical_accuracy: 0.7431 - val_loss: 0.0939 - val_f1_score: 0.4179 - val_categorical_accuracy: 0.6899\n",
      "Epoch 76/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0628 - f1_score: 0.5518 - categorical_accuracy: 0.7481 - val_loss: 0.0950 - val_f1_score: 0.4232 - val_categorical_accuracy: 0.6777\n",
      "Epoch 77/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0634 - f1_score: 0.5595 - categorical_accuracy: 0.7467 - val_loss: 0.0932 - val_f1_score: 0.4481 - val_categorical_accuracy: 0.7065\n",
      "Epoch 78/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0624 - f1_score: 0.5593 - categorical_accuracy: 0.7522 - val_loss: 0.0975 - val_f1_score: 0.4577 - val_categorical_accuracy: 0.6955\n",
      "Epoch 79/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0634 - f1_score: 0.5540 - categorical_accuracy: 0.7453 - val_loss: 0.0944 - val_f1_score: 0.4320 - val_categorical_accuracy: 0.6822\n",
      "Epoch 80/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0632 - f1_score: 0.5577 - categorical_accuracy: 0.7528 - val_loss: 0.0968 - val_f1_score: 0.4204 - val_categorical_accuracy: 0.6777\n",
      "Epoch 81/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0624 - f1_score: 0.5578 - categorical_accuracy: 0.7478 - val_loss: 0.0962 - val_f1_score: 0.4243 - val_categorical_accuracy: 0.6678\n",
      "Epoch 82/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0619 - f1_score: 0.5509 - categorical_accuracy: 0.7456 - val_loss: 0.0957 - val_f1_score: 0.4511 - val_categorical_accuracy: 0.6800\n",
      "Epoch 83/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0630 - f1_score: 0.5553 - categorical_accuracy: 0.7467 - val_loss: 0.0977 - val_f1_score: 0.4184 - val_categorical_accuracy: 0.6744\n",
      "Epoch 84/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0630 - f1_score: 0.5613 - categorical_accuracy: 0.7497 - val_loss: 0.0980 - val_f1_score: 0.4303 - val_categorical_accuracy: 0.6877\n",
      "Epoch 85/85\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 0.0618 - f1_score: 0.5580 - categorical_accuracy: 0.7511 - val_loss: 0.0974 - val_f1_score: 0.4413 - val_categorical_accuracy: 0.6777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x77993c707f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.2e-3\n",
    "EPOCHS=85\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.15, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 761us/step\n",
      "Training\n",
      "[[ 217  338   35]\n",
      " [ 129 2936  185]\n",
      " [  48  386  241]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.37      0.44       590\n",
      "         1.0       0.80      0.90      0.85      3250\n",
      "         2.0       0.52      0.36      0.42       675\n",
      "\n",
      "    accuracy                           0.75      4515\n",
      "   macro avg       0.63      0.54      0.57      4515\n",
      "weighted avg       0.73      0.75      0.73      4515\n",
      "\n",
      "21/21 [==============================] - 0s 788us/step\n",
      "Testing\n",
      "[[ 16  60   8]\n",
      " [ 36 384  45]\n",
      " [  6  69  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.19      0.23        84\n",
      "         1.0       0.75      0.83      0.79       465\n",
      "         2.0       0.28      0.22      0.25        96\n",
      "\n",
      "    accuracy                           0.65       645\n",
      "   macro avg       0.44      0.41      0.42       645\n",
      "weighted avg       0.62      0.65      0.63       645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(2)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d_6 (Separa  (None, 13, 32)            1782      \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 13, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_7 (Separa  (None, 11, 16)            624       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 11, 16)            64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_8 (Separa  (None, 9, 8)              184       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 9, 8)              32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "217/217 [==============================] - 1s 3ms/step - loss: 0.1160 - f1_score: 0.3197 - categorical_accuracy: 0.6639 - val_loss: 0.1062 - val_f1_score: 0.2762 - val_categorical_accuracy: 0.7074\n",
      "Epoch 2/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0985 - f1_score: 0.3032 - categorical_accuracy: 0.7166 - val_loss: 0.1025 - val_f1_score: 0.2761 - val_categorical_accuracy: 0.7062\n",
      "Epoch 3/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0956 - f1_score: 0.2938 - categorical_accuracy: 0.7137 - val_loss: 0.1012 - val_f1_score: 0.2829 - val_categorical_accuracy: 0.7108\n",
      "Epoch 4/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0942 - f1_score: 0.3018 - categorical_accuracy: 0.7151 - val_loss: 0.1007 - val_f1_score: 0.2876 - val_categorical_accuracy: 0.7108\n",
      "Epoch 5/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0926 - f1_score: 0.3064 - categorical_accuracy: 0.7172 - val_loss: 0.1004 - val_f1_score: 0.2875 - val_categorical_accuracy: 0.7108\n",
      "Epoch 6/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0913 - f1_score: 0.3224 - categorical_accuracy: 0.7189 - val_loss: 0.0996 - val_f1_score: 0.2915 - val_categorical_accuracy: 0.7039\n",
      "Epoch 7/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0900 - f1_score: 0.3320 - categorical_accuracy: 0.7203 - val_loss: 0.0996 - val_f1_score: 0.2899 - val_categorical_accuracy: 0.7016\n",
      "Epoch 8/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0894 - f1_score: 0.3442 - categorical_accuracy: 0.7189 - val_loss: 0.0994 - val_f1_score: 0.2836 - val_categorical_accuracy: 0.6993\n",
      "Epoch 9/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0882 - f1_score: 0.3400 - categorical_accuracy: 0.7177 - val_loss: 0.0990 - val_f1_score: 0.2967 - val_categorical_accuracy: 0.6970\n",
      "Epoch 10/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0864 - f1_score: 0.3589 - categorical_accuracy: 0.7241 - val_loss: 0.0993 - val_f1_score: 0.3149 - val_categorical_accuracy: 0.6809\n",
      "Epoch 11/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0857 - f1_score: 0.3686 - categorical_accuracy: 0.7241 - val_loss: 0.1004 - val_f1_score: 0.3360 - val_categorical_accuracy: 0.6832\n",
      "Epoch 12/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0849 - f1_score: 0.3708 - categorical_accuracy: 0.7195 - val_loss: 0.0985 - val_f1_score: 0.3398 - val_categorical_accuracy: 0.6970\n",
      "Epoch 13/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0835 - f1_score: 0.3923 - categorical_accuracy: 0.7198 - val_loss: 0.0981 - val_f1_score: 0.3462 - val_categorical_accuracy: 0.6912\n",
      "Epoch 14/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0823 - f1_score: 0.4140 - categorical_accuracy: 0.7284 - val_loss: 0.0995 - val_f1_score: 0.3494 - val_categorical_accuracy: 0.6774\n",
      "Epoch 15/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0821 - f1_score: 0.4058 - categorical_accuracy: 0.7221 - val_loss: 0.0987 - val_f1_score: 0.3759 - val_categorical_accuracy: 0.6855\n",
      "Epoch 16/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0810 - f1_score: 0.4297 - categorical_accuracy: 0.7270 - val_loss: 0.0994 - val_f1_score: 0.3547 - val_categorical_accuracy: 0.6878\n",
      "Epoch 17/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0805 - f1_score: 0.4376 - categorical_accuracy: 0.7281 - val_loss: 0.0988 - val_f1_score: 0.3452 - val_categorical_accuracy: 0.6935\n",
      "Epoch 18/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0801 - f1_score: 0.4391 - categorical_accuracy: 0.7272 - val_loss: 0.0996 - val_f1_score: 0.3730 - val_categorical_accuracy: 0.6774\n",
      "Epoch 19/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0793 - f1_score: 0.4402 - categorical_accuracy: 0.7290 - val_loss: 0.0990 - val_f1_score: 0.3992 - val_categorical_accuracy: 0.6797\n",
      "Epoch 20/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0791 - f1_score: 0.4534 - categorical_accuracy: 0.7304 - val_loss: 0.0983 - val_f1_score: 0.4278 - val_categorical_accuracy: 0.6924\n",
      "Epoch 21/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0777 - f1_score: 0.4592 - categorical_accuracy: 0.7290 - val_loss: 0.1000 - val_f1_score: 0.3714 - val_categorical_accuracy: 0.6809\n",
      "Epoch 22/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0768 - f1_score: 0.4780 - categorical_accuracy: 0.7336 - val_loss: 0.0981 - val_f1_score: 0.3801 - val_categorical_accuracy: 0.6774\n",
      "Epoch 23/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0772 - f1_score: 0.4544 - categorical_accuracy: 0.7275 - val_loss: 0.0982 - val_f1_score: 0.4074 - val_categorical_accuracy: 0.6970\n",
      "Epoch 24/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0760 - f1_score: 0.4869 - categorical_accuracy: 0.7347 - val_loss: 0.1004 - val_f1_score: 0.4160 - val_categorical_accuracy: 0.6774\n",
      "Epoch 25/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0745 - f1_score: 0.5064 - categorical_accuracy: 0.7402 - val_loss: 0.0993 - val_f1_score: 0.3998 - val_categorical_accuracy: 0.6820\n",
      "Epoch 26/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0746 - f1_score: 0.4919 - categorical_accuracy: 0.7359 - val_loss: 0.1005 - val_f1_score: 0.4121 - val_categorical_accuracy: 0.6866\n",
      "Epoch 27/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0753 - f1_score: 0.4911 - categorical_accuracy: 0.7324 - val_loss: 0.1008 - val_f1_score: 0.4290 - val_categorical_accuracy: 0.6786\n",
      "Epoch 28/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0739 - f1_score: 0.4924 - categorical_accuracy: 0.7310 - val_loss: 0.1002 - val_f1_score: 0.4258 - val_categorical_accuracy: 0.6601\n",
      "Epoch 29/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0740 - f1_score: 0.4934 - categorical_accuracy: 0.7342 - val_loss: 0.1013 - val_f1_score: 0.4040 - val_categorical_accuracy: 0.6590\n",
      "Epoch 30/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0730 - f1_score: 0.5186 - categorical_accuracy: 0.7411 - val_loss: 0.0983 - val_f1_score: 0.4272 - val_categorical_accuracy: 0.6878\n",
      "Epoch 31/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0725 - f1_score: 0.5075 - categorical_accuracy: 0.7382 - val_loss: 0.1008 - val_f1_score: 0.4044 - val_categorical_accuracy: 0.6555\n",
      "Epoch 32/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0716 - f1_score: 0.5284 - categorical_accuracy: 0.7399 - val_loss: 0.0981 - val_f1_score: 0.4084 - val_categorical_accuracy: 0.6682\n",
      "Epoch 33/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0724 - f1_score: 0.5187 - categorical_accuracy: 0.7376 - val_loss: 0.0995 - val_f1_score: 0.4354 - val_categorical_accuracy: 0.6694\n",
      "Epoch 34/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0710 - f1_score: 0.5359 - categorical_accuracy: 0.7414 - val_loss: 0.1000 - val_f1_score: 0.4025 - val_categorical_accuracy: 0.6682\n",
      "Epoch 35/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0706 - f1_score: 0.5217 - categorical_accuracy: 0.7388 - val_loss: 0.1017 - val_f1_score: 0.4419 - val_categorical_accuracy: 0.6786\n",
      "Epoch 36/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0702 - f1_score: 0.5298 - categorical_accuracy: 0.7425 - val_loss: 0.0998 - val_f1_score: 0.4030 - val_categorical_accuracy: 0.6740\n",
      "Epoch 37/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0693 - f1_score: 0.5335 - categorical_accuracy: 0.7411 - val_loss: 0.1012 - val_f1_score: 0.3948 - val_categorical_accuracy: 0.6601\n",
      "Epoch 38/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0710 - f1_score: 0.5346 - categorical_accuracy: 0.7422 - val_loss: 0.1001 - val_f1_score: 0.4165 - val_categorical_accuracy: 0.6694\n",
      "Epoch 39/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0694 - f1_score: 0.5429 - categorical_accuracy: 0.7445 - val_loss: 0.1004 - val_f1_score: 0.4159 - val_categorical_accuracy: 0.6717\n",
      "Epoch 40/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0697 - f1_score: 0.5375 - categorical_accuracy: 0.7437 - val_loss: 0.1027 - val_f1_score: 0.3978 - val_categorical_accuracy: 0.6486\n",
      "Epoch 41/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0690 - f1_score: 0.5414 - categorical_accuracy: 0.7431 - val_loss: 0.1014 - val_f1_score: 0.4034 - val_categorical_accuracy: 0.6613\n",
      "Epoch 42/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0682 - f1_score: 0.5588 - categorical_accuracy: 0.7529 - val_loss: 0.1020 - val_f1_score: 0.3944 - val_categorical_accuracy: 0.6486\n",
      "Epoch 43/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0682 - f1_score: 0.5496 - categorical_accuracy: 0.7448 - val_loss: 0.1020 - val_f1_score: 0.4178 - val_categorical_accuracy: 0.6509\n",
      "Epoch 44/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0687 - f1_score: 0.5482 - categorical_accuracy: 0.7451 - val_loss: 0.1020 - val_f1_score: 0.4273 - val_categorical_accuracy: 0.6647\n",
      "Epoch 45/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0675 - f1_score: 0.5542 - categorical_accuracy: 0.7488 - val_loss: 0.1011 - val_f1_score: 0.4256 - val_categorical_accuracy: 0.6578\n",
      "Epoch 46/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0670 - f1_score: 0.5461 - categorical_accuracy: 0.7445 - val_loss: 0.1027 - val_f1_score: 0.4456 - val_categorical_accuracy: 0.6613\n",
      "Epoch 47/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0679 - f1_score: 0.5510 - categorical_accuracy: 0.7416 - val_loss: 0.1021 - val_f1_score: 0.3971 - val_categorical_accuracy: 0.6486\n",
      "Epoch 48/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0660 - f1_score: 0.5528 - categorical_accuracy: 0.7451 - val_loss: 0.1033 - val_f1_score: 0.4169 - val_categorical_accuracy: 0.6590\n",
      "Epoch 49/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0663 - f1_score: 0.5476 - categorical_accuracy: 0.7419 - val_loss: 0.1034 - val_f1_score: 0.3951 - val_categorical_accuracy: 0.6406\n",
      "Epoch 50/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0667 - f1_score: 0.5448 - categorical_accuracy: 0.7416 - val_loss: 0.1031 - val_f1_score: 0.3899 - val_categorical_accuracy: 0.6429\n",
      "Epoch 51/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0668 - f1_score: 0.5575 - categorical_accuracy: 0.7468 - val_loss: 0.1037 - val_f1_score: 0.4088 - val_categorical_accuracy: 0.6578\n",
      "Epoch 52/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0664 - f1_score: 0.5646 - categorical_accuracy: 0.7512 - val_loss: 0.1057 - val_f1_score: 0.3904 - val_categorical_accuracy: 0.6348\n",
      "Epoch 53/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0663 - f1_score: 0.5537 - categorical_accuracy: 0.7445 - val_loss: 0.1029 - val_f1_score: 0.4056 - val_categorical_accuracy: 0.6521\n",
      "Epoch 54/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0658 - f1_score: 0.5611 - categorical_accuracy: 0.7474 - val_loss: 0.1032 - val_f1_score: 0.4019 - val_categorical_accuracy: 0.6555\n",
      "Epoch 55/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0648 - f1_score: 0.5533 - categorical_accuracy: 0.7445 - val_loss: 0.1028 - val_f1_score: 0.4267 - val_categorical_accuracy: 0.6509\n",
      "Epoch 56/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0649 - f1_score: 0.5660 - categorical_accuracy: 0.7491 - val_loss: 0.1069 - val_f1_score: 0.4011 - val_categorical_accuracy: 0.6302\n",
      "Epoch 57/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0656 - f1_score: 0.5709 - categorical_accuracy: 0.7488 - val_loss: 0.1056 - val_f1_score: 0.3887 - val_categorical_accuracy: 0.6233\n",
      "Epoch 58/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0648 - f1_score: 0.5709 - categorical_accuracy: 0.7488 - val_loss: 0.1072 - val_f1_score: 0.3842 - val_categorical_accuracy: 0.6210\n",
      "Epoch 59/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0645 - f1_score: 0.5786 - categorical_accuracy: 0.7552 - val_loss: 0.1072 - val_f1_score: 0.3851 - val_categorical_accuracy: 0.6313\n",
      "Epoch 60/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0632 - f1_score: 0.5671 - categorical_accuracy: 0.7486 - val_loss: 0.1070 - val_f1_score: 0.3961 - val_categorical_accuracy: 0.6359\n",
      "Epoch 61/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0638 - f1_score: 0.5621 - categorical_accuracy: 0.7448 - val_loss: 0.1074 - val_f1_score: 0.4156 - val_categorical_accuracy: 0.6521\n",
      "Epoch 62/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0639 - f1_score: 0.5660 - categorical_accuracy: 0.7506 - val_loss: 0.1093 - val_f1_score: 0.3966 - val_categorical_accuracy: 0.6382\n",
      "Epoch 63/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0642 - f1_score: 0.5576 - categorical_accuracy: 0.7434 - val_loss: 0.1073 - val_f1_score: 0.4087 - val_categorical_accuracy: 0.6498\n",
      "Epoch 64/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0634 - f1_score: 0.5697 - categorical_accuracy: 0.7457 - val_loss: 0.1102 - val_f1_score: 0.3798 - val_categorical_accuracy: 0.6256\n",
      "Epoch 65/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0637 - f1_score: 0.5674 - categorical_accuracy: 0.7477 - val_loss: 0.1113 - val_f1_score: 0.4069 - val_categorical_accuracy: 0.6440\n",
      "Epoch 66/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0634 - f1_score: 0.5706 - categorical_accuracy: 0.7503 - val_loss: 0.1088 - val_f1_score: 0.4060 - val_categorical_accuracy: 0.6256\n",
      "Epoch 67/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0623 - f1_score: 0.5869 - categorical_accuracy: 0.7543 - val_loss: 0.1110 - val_f1_score: 0.4017 - val_categorical_accuracy: 0.6256\n",
      "Epoch 68/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0628 - f1_score: 0.5757 - categorical_accuracy: 0.7535 - val_loss: 0.1080 - val_f1_score: 0.4114 - val_categorical_accuracy: 0.6336\n",
      "Epoch 69/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0626 - f1_score: 0.5852 - categorical_accuracy: 0.7569 - val_loss: 0.1085 - val_f1_score: 0.4083 - val_categorical_accuracy: 0.6348\n",
      "Epoch 70/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0628 - f1_score: 0.5717 - categorical_accuracy: 0.7463 - val_loss: 0.1134 - val_f1_score: 0.3843 - val_categorical_accuracy: 0.6106\n",
      "Epoch 71/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0626 - f1_score: 0.5758 - categorical_accuracy: 0.7503 - val_loss: 0.1129 - val_f1_score: 0.3866 - val_categorical_accuracy: 0.6175\n",
      "Epoch 72/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0625 - f1_score: 0.5705 - categorical_accuracy: 0.7497 - val_loss: 0.1123 - val_f1_score: 0.3965 - val_categorical_accuracy: 0.6302\n",
      "Epoch 73/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0624 - f1_score: 0.5719 - categorical_accuracy: 0.7529 - val_loss: 0.1138 - val_f1_score: 0.4058 - val_categorical_accuracy: 0.6221\n",
      "Epoch 74/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0622 - f1_score: 0.5799 - categorical_accuracy: 0.7500 - val_loss: 0.1149 - val_f1_score: 0.3860 - val_categorical_accuracy: 0.6290\n",
      "Epoch 75/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0620 - f1_score: 0.5719 - categorical_accuracy: 0.7503 - val_loss: 0.1130 - val_f1_score: 0.3824 - val_categorical_accuracy: 0.6175\n",
      "Epoch 76/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0620 - f1_score: 0.5668 - categorical_accuracy: 0.7451 - val_loss: 0.1135 - val_f1_score: 0.3826 - val_categorical_accuracy: 0.6071\n",
      "Epoch 77/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0607 - f1_score: 0.5852 - categorical_accuracy: 0.7532 - val_loss: 0.1143 - val_f1_score: 0.3738 - val_categorical_accuracy: 0.6187\n",
      "Epoch 78/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0605 - f1_score: 0.5830 - categorical_accuracy: 0.7578 - val_loss: 0.1155 - val_f1_score: 0.3861 - val_categorical_accuracy: 0.6187\n",
      "Epoch 79/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0614 - f1_score: 0.5777 - categorical_accuracy: 0.7486 - val_loss: 0.1165 - val_f1_score: 0.3903 - val_categorical_accuracy: 0.6198\n",
      "Epoch 80/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0617 - f1_score: 0.5759 - categorical_accuracy: 0.7488 - val_loss: 0.1128 - val_f1_score: 0.3872 - val_categorical_accuracy: 0.6210\n",
      "Epoch 81/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0607 - f1_score: 0.5725 - categorical_accuracy: 0.7512 - val_loss: 0.1152 - val_f1_score: 0.3876 - val_categorical_accuracy: 0.6118\n",
      "Epoch 82/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0604 - f1_score: 0.5900 - categorical_accuracy: 0.7560 - val_loss: 0.1148 - val_f1_score: 0.3819 - val_categorical_accuracy: 0.6164\n",
      "Epoch 83/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0597 - f1_score: 0.5848 - categorical_accuracy: 0.7560 - val_loss: 0.1174 - val_f1_score: 0.3953 - val_categorical_accuracy: 0.6037\n",
      "Epoch 84/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0598 - f1_score: 0.5826 - categorical_accuracy: 0.7543 - val_loss: 0.1184 - val_f1_score: 0.3744 - val_categorical_accuracy: 0.6187\n",
      "Epoch 85/85\n",
      "217/217 [==============================] - 0s 2ms/step - loss: 0.0608 - f1_score: 0.5666 - categorical_accuracy: 0.7454 - val_loss: 0.1200 - val_f1_score: 0.3755 - val_categorical_accuracy: 0.6129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x77996c1e1840>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.2e-3\n",
    "EPOCHS=85\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.15, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 835us/step\n",
      "Training\n",
      "[[ 210  310   48]\n",
      " [ 140 2770  187]\n",
      " [  40  377  258]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.37      0.44       568\n",
      "         1.0       0.80      0.89      0.85      3097\n",
      "         2.0       0.52      0.38      0.44       675\n",
      "\n",
      "    accuracy                           0.75      4340\n",
      "   macro avg       0.62      0.55      0.58      4340\n",
      "weighted avg       0.72      0.75      0.73      4340\n",
      "\n",
      "20/20 [==============================] - 0s 811us/step\n",
      "Testing\n",
      "[[ 21  56   4]\n",
      " [ 45 352  46]\n",
      " [  6  74  16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.26      0.27        81\n",
      "         1.0       0.73      0.79      0.76       443\n",
      "         2.0       0.24      0.17      0.20        96\n",
      "\n",
      "    accuracy                           0.63       620\n",
      "   macro avg       0.42      0.41      0.41       620\n",
      "weighted avg       0.60      0.63      0.61       620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandB Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pick the best possible params for the Conv-1D model, since randomly trying params wasn't fruitful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from wandb.keras import WandbCallback, WandbMetricsLogger\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2\n",
    "MOMENTUM = 0.9\n",
    "X_train_50 = None\n",
    "y_train_onehot = None\n",
    "\n",
    "def def_model():\n",
    "    inputs = keras.Input(shape=(750,))\n",
    "    x = layers.Reshape((15, 50))(inputs)\n",
    "    x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(10, activation=keras.layers.LeakyReLU(),\n",
    "                    kernel_initializer=keras.initializers.GlorotNormal(),\n",
    "                    bias_initializer=keras.initializers.Zeros())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    global X_train_50, y_train_onehot\n",
    "    pca_x, olr_labels = load_pca_anomaly()\n",
    "    pca_x_50 = pca_x[:, :50]\n",
    "    pca_x_50 = np.array([pca_x_50[i*40+j:i*40+j+15, :] for j in range(135) for i in range(40)])\n",
    "    olr_labels = np.reshape(olr_labels, -1)\n",
    "    X_train, _, y_train, _ = train_test_split(pca_x_50, olr_labels, random_state=1337, train_size=0.875, stratify=olr_labels)\n",
    "    X_train_50 = np.reshape(X_train, (4725, -1))\n",
    "    y_train_onehot = to_categorical(y_train)\n",
    "\n",
    "\n",
    "def get_optimizer(lr=1e-3, optimizer=\"adam\"):\n",
    "    if optimizer.lower() == \"adam\":\n",
    "        return keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer.lower() == \"sgd\":\n",
    "        return keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    if optimizer.lower() == \"nesterov\":\n",
    "        return keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "    if optimizer.lower() == \"rmsprop\":\n",
    "        return keras.optimizers.RMSprop(earning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "def train(model, batch_size=64, epochs=10, lr=1e-3, optimizer='rmsprop', alpha=0.25, gamma=2, log_freq=10):\n",
    "    global X_train_50, y_train_onehot\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.compile(loss=keras.losses.CategoricalFocalCrossentropy(alpha=alpha, gamma=gamma), \n",
    "                  optimizer=get_optimizer(lr, optimizer), \n",
    "                  metrics=  [\n",
    "                                keras.metrics.F1Score(average='macro'),\n",
    "                                keras.metrics.CategoricalAccuracy(),\n",
    "                            ])\n",
    "    TIMESTAMP = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    mydir = os.path.join(os.getcwd(), f\"logs/logdir_{TIMESTAMP}\")\n",
    "    os.makedirs(mydir)\n",
    "    callbacks=[\n",
    "                keras.callbacks.TensorBoard(log_dir=mydir),\n",
    "                WandbCallback(log_gradients=True, training_data=(X_train_50, y_train_onehot)),\n",
    "                WandbMetricsLogger(log_freq=log_freq)\n",
    "            ]\n",
    "\n",
    "    model.fit(X_train, \n",
    "              y_train_onehot, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_split=0.1, \n",
    "              callbacks=callbacks)\n",
    "    \n",
    "\n",
    "def sweep_train(config_defaults=None):\n",
    "    with wandb.init(config=config_defaults):\n",
    "        wandb.config.architecture_name = \"Conv-1D\"\n",
    "        wandb.config.dataset_name = \"OLR\"\n",
    "\n",
    "        model = def_model()\n",
    "\n",
    "        train(model, \n",
    "              wandb.config.batch_size, \n",
    "              wandb.config.epochs,\n",
    "              wandb.config.lr,\n",
    "              wandb.config.optimizer,\n",
    "              wandb.config.alpha,\n",
    "              wandb.config.gamma)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data()\n",
    "    wandb.login()\n",
    "    sweep_configuration = {\n",
    "        \"method\": \"random\",\n",
    "        \"name\": \"sweep_rms\",\n",
    "        \"metric\":   {\n",
    "                        \"goal\": \"maximize\", \n",
    "                        \"name\": \"val_f1_score\"\n",
    "                    },  \n",
    "        \"parameters\":   {\n",
    "                            \"batch_size\":   {\n",
    "                                                \"values\": [8, 16, 32, 64, 128]\n",
    "                                            },\n",
    "                            \"epochs\":   {\n",
    "                                            \"distribution\": \"int_uniform\",\n",
    "                                            \"max\": 400,\n",
    "                                            \"min\": 15\n",
    "                                        },\n",
    "                            \"lr\":   {\n",
    "                                        \"distribution\": \"uniform\",\n",
    "                                        \"max\": 1e-2, \n",
    "                                        \"min\": 1e-6\n",
    "                                    },\n",
    "                            \"optimizer\":    {\n",
    "                                                \"values\": [\"sgd\", \"nesterov\", \"adam\", \"rmsprop\"],\n",
    "                                            },\n",
    "                            \"alpha\":{\n",
    "                                        \"values\": [0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "                                    },\n",
    "                            \"gamma\":{\n",
    "                                        \"values\": [0.5, 1, 2, 3, 4, 5]\n",
    "                                    },\n",
    "                        },\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"OLR_Base_Model\")\n",
    "    wandb.agent(sweep_id, function=sweep_train, count=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
