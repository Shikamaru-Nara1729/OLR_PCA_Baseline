{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/apoorva/Desktop/Work/olr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 19:57:32.288256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.utils.load import load_pca_anomaly\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_x, olr_labels = load_pca_anomaly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pentad_data(count):\n",
    "    '''\n",
    "    count is 0-indexed\n",
    "    count = 0 corresponds to first leading pentad\n",
    "    count = 1 corresponds to second leading pentad\n",
    "    count = 2 corresponds to third leading pentad\n",
    "    '''\n",
    "    global olr_labels, pca_x\n",
    "    assert count == 0 or count == 1 or count == 2\n",
    "    pca_x_50 = pca_x[:, :50]\n",
    "    pca_x_50 = np.array([pca_x_50[i*40+j:i*40+j+15, :] for j in range(134 - (5*count)) for i in range(40)])\n",
    "    labels = np.reshape(np.reshape(olr_labels, (40, 135))[:, 1+(5*count):], (-1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pca_x_50, labels, random_state=1337, train_size=0.875, stratify=labels)\n",
    "    # X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    # X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d_3 (Separa  (None, 13, 32)            1782      \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 13, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_4 (Separa  (None, 11, 16)            624       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 11, 16)            64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_5 (Separa  (None, 9, 8)              184       \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 9, 8)              32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "235/235 [==============================] - 5s 8ms/step - loss: 0.2127 - f1_score: 0.2995 - categorical_accuracy: 0.6911 - val_loss: 0.1879 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1927 - f1_score: 0.2798 - categorical_accuracy: 0.7231 - val_loss: 0.1888 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1902 - f1_score: 0.2798 - categorical_accuracy: 0.7231 - val_loss: 0.1888 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1886 - f1_score: 0.2798 - categorical_accuracy: 0.7231 - val_loss: 0.1866 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1863 - f1_score: 0.2798 - categorical_accuracy: 0.7231 - val_loss: 0.1907 - val_f1_score: 0.2821 - val_categorical_accuracy: 0.7335\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1842 - f1_score: 0.2797 - categorical_accuracy: 0.7228 - val_loss: 0.1901 - val_f1_score: 0.2818 - val_categorical_accuracy: 0.7324\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1822 - f1_score: 0.2823 - categorical_accuracy: 0.7231 - val_loss: 0.1914 - val_f1_score: 0.2818 - val_categorical_accuracy: 0.7324\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1796 - f1_score: 0.2927 - categorical_accuracy: 0.7244 - val_loss: 0.1966 - val_f1_score: 0.2809 - val_categorical_accuracy: 0.7281\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1768 - f1_score: 0.3016 - categorical_accuracy: 0.7255 - val_loss: 0.1987 - val_f1_score: 0.2911 - val_categorical_accuracy: 0.7271\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1751 - f1_score: 0.3068 - categorical_accuracy: 0.7231 - val_loss: 0.1996 - val_f1_score: 0.2926 - val_categorical_accuracy: 0.7313\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1728 - f1_score: 0.3119 - categorical_accuracy: 0.7247 - val_loss: 0.2030 - val_f1_score: 0.2965 - val_categorical_accuracy: 0.7122\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1720 - f1_score: 0.3276 - categorical_accuracy: 0.7223 - val_loss: 0.2026 - val_f1_score: 0.2946 - val_categorical_accuracy: 0.7217\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1689 - f1_score: 0.3372 - categorical_accuracy: 0.7241 - val_loss: 0.2079 - val_f1_score: 0.2880 - val_categorical_accuracy: 0.7154\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1675 - f1_score: 0.3633 - categorical_accuracy: 0.7255 - val_loss: 0.2093 - val_f1_score: 0.3111 - val_categorical_accuracy: 0.7068\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1657 - f1_score: 0.3516 - categorical_accuracy: 0.7252 - val_loss: 0.2124 - val_f1_score: 0.2962 - val_categorical_accuracy: 0.6962\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1632 - f1_score: 0.3812 - categorical_accuracy: 0.7284 - val_loss: 0.2159 - val_f1_score: 0.3092 - val_categorical_accuracy: 0.6962\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1624 - f1_score: 0.3946 - categorical_accuracy: 0.7303 - val_loss: 0.2162 - val_f1_score: 0.2892 - val_categorical_accuracy: 0.6994\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1598 - f1_score: 0.3973 - categorical_accuracy: 0.7324 - val_loss: 0.2181 - val_f1_score: 0.3033 - val_categorical_accuracy: 0.6908\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1584 - f1_score: 0.4194 - categorical_accuracy: 0.7297 - val_loss: 0.2196 - val_f1_score: 0.3046 - val_categorical_accuracy: 0.6951\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1566 - f1_score: 0.4138 - categorical_accuracy: 0.7305 - val_loss: 0.2197 - val_f1_score: 0.3152 - val_categorical_accuracy: 0.6919\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1555 - f1_score: 0.4331 - categorical_accuracy: 0.7343 - val_loss: 0.2243 - val_f1_score: 0.3010 - val_categorical_accuracy: 0.6962\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1534 - f1_score: 0.4313 - categorical_accuracy: 0.7297 - val_loss: 0.2257 - val_f1_score: 0.3098 - val_categorical_accuracy: 0.6866\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1526 - f1_score: 0.4414 - categorical_accuracy: 0.7319 - val_loss: 0.2260 - val_f1_score: 0.3157 - val_categorical_accuracy: 0.6844\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1502 - f1_score: 0.4590 - categorical_accuracy: 0.7383 - val_loss: 0.2353 - val_f1_score: 0.2979 - val_categorical_accuracy: 0.6866\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1493 - f1_score: 0.4546 - categorical_accuracy: 0.7303 - val_loss: 0.2320 - val_f1_score: 0.3068 - val_categorical_accuracy: 0.6908\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1489 - f1_score: 0.4445 - categorical_accuracy: 0.7255 - val_loss: 0.2355 - val_f1_score: 0.3048 - val_categorical_accuracy: 0.6631\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1461 - f1_score: 0.4671 - categorical_accuracy: 0.7319 - val_loss: 0.2352 - val_f1_score: 0.3215 - val_categorical_accuracy: 0.6780\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1452 - f1_score: 0.4737 - categorical_accuracy: 0.7324 - val_loss: 0.2383 - val_f1_score: 0.3280 - val_categorical_accuracy: 0.6695\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1435 - f1_score: 0.4752 - categorical_accuracy: 0.7345 - val_loss: 0.2469 - val_f1_score: 0.3148 - val_categorical_accuracy: 0.6706\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1441 - f1_score: 0.4747 - categorical_accuracy: 0.7311 - val_loss: 0.2403 - val_f1_score: 0.3201 - val_categorical_accuracy: 0.6748\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1413 - f1_score: 0.4944 - categorical_accuracy: 0.7396 - val_loss: 0.2484 - val_f1_score: 0.3162 - val_categorical_accuracy: 0.6482\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1406 - f1_score: 0.4913 - categorical_accuracy: 0.7380 - val_loss: 0.2519 - val_f1_score: 0.3104 - val_categorical_accuracy: 0.6482\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1399 - f1_score: 0.5058 - categorical_accuracy: 0.7396 - val_loss: 0.2529 - val_f1_score: 0.3237 - val_categorical_accuracy: 0.6482\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1391 - f1_score: 0.4937 - categorical_accuracy: 0.7308 - val_loss: 0.2533 - val_f1_score: 0.3014 - val_categorical_accuracy: 0.6620\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1397 - f1_score: 0.4769 - categorical_accuracy: 0.7327 - val_loss: 0.2515 - val_f1_score: 0.3379 - val_categorical_accuracy: 0.6493\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1370 - f1_score: 0.5081 - categorical_accuracy: 0.7375 - val_loss: 0.2585 - val_f1_score: 0.3159 - val_categorical_accuracy: 0.6642\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1359 - f1_score: 0.4952 - categorical_accuracy: 0.7359 - val_loss: 0.2595 - val_f1_score: 0.3243 - val_categorical_accuracy: 0.6333\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1353 - f1_score: 0.5003 - categorical_accuracy: 0.7319 - val_loss: 0.2634 - val_f1_score: 0.3296 - val_categorical_accuracy: 0.6599\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1350 - f1_score: 0.5092 - categorical_accuracy: 0.7380 - val_loss: 0.2619 - val_f1_score: 0.3272 - val_categorical_accuracy: 0.6397\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1339 - f1_score: 0.5014 - categorical_accuracy: 0.7377 - val_loss: 0.2585 - val_f1_score: 0.3516 - val_categorical_accuracy: 0.6397\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1334 - f1_score: 0.5218 - categorical_accuracy: 0.7401 - val_loss: 0.2662 - val_f1_score: 0.3226 - val_categorical_accuracy: 0.6258\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1321 - f1_score: 0.5336 - categorical_accuracy: 0.7441 - val_loss: 0.2688 - val_f1_score: 0.3286 - val_categorical_accuracy: 0.6429\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1326 - f1_score: 0.5245 - categorical_accuracy: 0.7420 - val_loss: 0.2693 - val_f1_score: 0.3161 - val_categorical_accuracy: 0.6461\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1318 - f1_score: 0.5162 - categorical_accuracy: 0.7388 - val_loss: 0.2676 - val_f1_score: 0.3549 - val_categorical_accuracy: 0.6471\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1300 - f1_score: 0.5293 - categorical_accuracy: 0.7420 - val_loss: 0.2693 - val_f1_score: 0.3381 - val_categorical_accuracy: 0.6482\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1294 - f1_score: 0.5299 - categorical_accuracy: 0.7431 - val_loss: 0.2716 - val_f1_score: 0.3350 - val_categorical_accuracy: 0.6269\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1294 - f1_score: 0.5208 - categorical_accuracy: 0.7335 - val_loss: 0.2716 - val_f1_score: 0.3266 - val_categorical_accuracy: 0.6343\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1283 - f1_score: 0.5340 - categorical_accuracy: 0.7415 - val_loss: 0.2734 - val_f1_score: 0.3238 - val_categorical_accuracy: 0.6301\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1284 - f1_score: 0.5244 - categorical_accuracy: 0.7401 - val_loss: 0.2708 - val_f1_score: 0.3365 - val_categorical_accuracy: 0.6290\n",
      "Epoch 50/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1287 - f1_score: 0.5304 - categorical_accuracy: 0.7396 - val_loss: 0.2783 - val_f1_score: 0.3241 - val_categorical_accuracy: 0.6205\n",
      "Epoch 51/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1270 - f1_score: 0.5331 - categorical_accuracy: 0.7428 - val_loss: 0.2792 - val_f1_score: 0.3428 - val_categorical_accuracy: 0.6205\n",
      "Epoch 52/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1265 - f1_score: 0.5477 - categorical_accuracy: 0.7455 - val_loss: 0.2809 - val_f1_score: 0.3167 - val_categorical_accuracy: 0.6365\n",
      "Epoch 53/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1260 - f1_score: 0.5197 - categorical_accuracy: 0.7359 - val_loss: 0.2804 - val_f1_score: 0.3314 - val_categorical_accuracy: 0.6301\n",
      "Epoch 54/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1251 - f1_score: 0.5363 - categorical_accuracy: 0.7436 - val_loss: 0.2757 - val_f1_score: 0.3470 - val_categorical_accuracy: 0.6247\n",
      "Epoch 55/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1252 - f1_score: 0.5389 - categorical_accuracy: 0.7428 - val_loss: 0.2859 - val_f1_score: 0.3248 - val_categorical_accuracy: 0.6173\n",
      "Epoch 56/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1250 - f1_score: 0.5352 - categorical_accuracy: 0.7423 - val_loss: 0.2831 - val_f1_score: 0.3478 - val_categorical_accuracy: 0.6279\n",
      "Epoch 57/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1254 - f1_score: 0.5407 - categorical_accuracy: 0.7401 - val_loss: 0.2795 - val_f1_score: 0.3524 - val_categorical_accuracy: 0.6279\n",
      "Epoch 58/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1249 - f1_score: 0.5336 - categorical_accuracy: 0.7425 - val_loss: 0.2873 - val_f1_score: 0.3427 - val_categorical_accuracy: 0.6269\n",
      "Epoch 59/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1243 - f1_score: 0.5399 - categorical_accuracy: 0.7407 - val_loss: 0.2868 - val_f1_score: 0.3487 - val_categorical_accuracy: 0.6514\n",
      "Epoch 60/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1233 - f1_score: 0.5381 - categorical_accuracy: 0.7436 - val_loss: 0.2910 - val_f1_score: 0.3534 - val_categorical_accuracy: 0.6173\n",
      "Epoch 61/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1239 - f1_score: 0.5439 - categorical_accuracy: 0.7441 - val_loss: 0.2902 - val_f1_score: 0.3597 - val_categorical_accuracy: 0.6173\n",
      "Epoch 62/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1235 - f1_score: 0.5317 - categorical_accuracy: 0.7367 - val_loss: 0.2882 - val_f1_score: 0.3397 - val_categorical_accuracy: 0.6258\n",
      "Epoch 63/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1220 - f1_score: 0.5485 - categorical_accuracy: 0.7417 - val_loss: 0.2898 - val_f1_score: 0.3418 - val_categorical_accuracy: 0.6247\n",
      "Epoch 64/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1221 - f1_score: 0.5571 - categorical_accuracy: 0.7527 - val_loss: 0.2909 - val_f1_score: 0.3599 - val_categorical_accuracy: 0.6247\n",
      "Epoch 65/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1222 - f1_score: 0.5434 - categorical_accuracy: 0.7449 - val_loss: 0.2898 - val_f1_score: 0.3324 - val_categorical_accuracy: 0.6343\n",
      "Epoch 66/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1227 - f1_score: 0.5564 - categorical_accuracy: 0.7471 - val_loss: 0.2934 - val_f1_score: 0.3568 - val_categorical_accuracy: 0.6279\n",
      "Epoch 67/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1208 - f1_score: 0.5449 - categorical_accuracy: 0.7425 - val_loss: 0.2957 - val_f1_score: 0.3487 - val_categorical_accuracy: 0.6258\n",
      "Epoch 68/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1201 - f1_score: 0.5704 - categorical_accuracy: 0.7532 - val_loss: 0.2984 - val_f1_score: 0.3452 - val_categorical_accuracy: 0.6183\n",
      "Epoch 69/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1205 - f1_score: 0.5570 - categorical_accuracy: 0.7487 - val_loss: 0.3049 - val_f1_score: 0.3354 - val_categorical_accuracy: 0.6226\n",
      "Epoch 70/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1209 - f1_score: 0.5475 - categorical_accuracy: 0.7444 - val_loss: 0.2975 - val_f1_score: 0.3554 - val_categorical_accuracy: 0.6130\n",
      "Epoch 71/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1199 - f1_score: 0.5600 - categorical_accuracy: 0.7489 - val_loss: 0.3011 - val_f1_score: 0.3489 - val_categorical_accuracy: 0.6247\n",
      "Epoch 72/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1212 - f1_score: 0.5450 - categorical_accuracy: 0.7423 - val_loss: 0.2946 - val_f1_score: 0.3483 - val_categorical_accuracy: 0.6290\n",
      "Epoch 73/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1187 - f1_score: 0.5593 - categorical_accuracy: 0.7471 - val_loss: 0.2991 - val_f1_score: 0.3173 - val_categorical_accuracy: 0.6365\n",
      "Epoch 74/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1199 - f1_score: 0.5411 - categorical_accuracy: 0.7415 - val_loss: 0.3013 - val_f1_score: 0.3618 - val_categorical_accuracy: 0.6226\n",
      "Epoch 75/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1187 - f1_score: 0.5557 - categorical_accuracy: 0.7484 - val_loss: 0.2939 - val_f1_score: 0.3533 - val_categorical_accuracy: 0.6215\n",
      "Epoch 76/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1196 - f1_score: 0.5361 - categorical_accuracy: 0.7335 - val_loss: 0.3042 - val_f1_score: 0.3430 - val_categorical_accuracy: 0.6279\n",
      "Epoch 77/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1183 - f1_score: 0.5551 - categorical_accuracy: 0.7452 - val_loss: 0.2975 - val_f1_score: 0.3449 - val_categorical_accuracy: 0.6354\n",
      "Epoch 78/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1180 - f1_score: 0.5566 - categorical_accuracy: 0.7479 - val_loss: 0.3051 - val_f1_score: 0.3507 - val_categorical_accuracy: 0.6098\n",
      "Epoch 79/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1190 - f1_score: 0.5428 - categorical_accuracy: 0.7393 - val_loss: 0.3051 - val_f1_score: 0.3339 - val_categorical_accuracy: 0.6162\n",
      "Epoch 80/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1174 - f1_score: 0.5720 - categorical_accuracy: 0.7553 - val_loss: 0.3075 - val_f1_score: 0.3325 - val_categorical_accuracy: 0.6279\n",
      "Epoch 81/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1183 - f1_score: 0.5511 - categorical_accuracy: 0.7449 - val_loss: 0.3074 - val_f1_score: 0.3329 - val_categorical_accuracy: 0.6205\n",
      "Epoch 82/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1166 - f1_score: 0.5523 - categorical_accuracy: 0.7420 - val_loss: 0.3038 - val_f1_score: 0.3647 - val_categorical_accuracy: 0.6343\n",
      "Epoch 83/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1177 - f1_score: 0.5553 - categorical_accuracy: 0.7484 - val_loss: 0.3096 - val_f1_score: 0.3603 - val_categorical_accuracy: 0.6023\n",
      "Epoch 84/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1187 - f1_score: 0.5553 - categorical_accuracy: 0.7455 - val_loss: 0.3092 - val_f1_score: 0.3425 - val_categorical_accuracy: 0.6290\n",
      "Epoch 85/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1160 - f1_score: 0.5610 - categorical_accuracy: 0.7521 - val_loss: 0.3166 - val_f1_score: 0.3448 - val_categorical_accuracy: 0.6077\n",
      "Epoch 86/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1182 - f1_score: 0.5380 - categorical_accuracy: 0.7383 - val_loss: 0.3096 - val_f1_score: 0.3302 - val_categorical_accuracy: 0.5917\n",
      "Epoch 87/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1170 - f1_score: 0.5530 - categorical_accuracy: 0.7452 - val_loss: 0.3141 - val_f1_score: 0.3542 - val_categorical_accuracy: 0.6098\n",
      "Epoch 88/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1164 - f1_score: 0.5557 - categorical_accuracy: 0.7487 - val_loss: 0.3167 - val_f1_score: 0.3436 - val_categorical_accuracy: 0.5949\n",
      "Epoch 89/100\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1156 - f1_score: 0.5646 - categorical_accuracy: 0.7500 - val_loss: 0.3232 - val_f1_score: 0.3280 - val_categorical_accuracy: 0.5789\n",
      "Epoch 90/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1148 - f1_score: 0.5656 - categorical_accuracy: 0.7481 - val_loss: 0.3233 - val_f1_score: 0.3424 - val_categorical_accuracy: 0.5981\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1153 - f1_score: 0.5647 - categorical_accuracy: 0.7476 - val_loss: 0.3273 - val_f1_score: 0.3495 - val_categorical_accuracy: 0.6183\n",
      "Epoch 92/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1143 - f1_score: 0.5613 - categorical_accuracy: 0.7513 - val_loss: 0.3243 - val_f1_score: 0.3403 - val_categorical_accuracy: 0.5928\n",
      "Epoch 93/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1165 - f1_score: 0.5520 - categorical_accuracy: 0.7407 - val_loss: 0.3207 - val_f1_score: 0.3473 - val_categorical_accuracy: 0.6077\n",
      "Epoch 94/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1154 - f1_score: 0.5587 - categorical_accuracy: 0.7476 - val_loss: 0.3179 - val_f1_score: 0.3437 - val_categorical_accuracy: 0.6279\n",
      "Epoch 95/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1153 - f1_score: 0.5534 - categorical_accuracy: 0.7457 - val_loss: 0.3229 - val_f1_score: 0.3337 - val_categorical_accuracy: 0.6173\n",
      "Epoch 96/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1165 - f1_score: 0.5440 - categorical_accuracy: 0.7412 - val_loss: 0.3213 - val_f1_score: 0.3408 - val_categorical_accuracy: 0.5991\n",
      "Epoch 97/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1151 - f1_score: 0.5615 - categorical_accuracy: 0.7473 - val_loss: 0.3151 - val_f1_score: 0.3479 - val_categorical_accuracy: 0.6002\n",
      "Epoch 98/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1133 - f1_score: 0.5617 - categorical_accuracy: 0.7489 - val_loss: 0.3249 - val_f1_score: 0.3427 - val_categorical_accuracy: 0.6034\n",
      "Epoch 99/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1138 - f1_score: 0.5623 - categorical_accuracy: 0.7503 - val_loss: 0.3218 - val_f1_score: 0.3659 - val_categorical_accuracy: 0.6151\n",
      "Epoch 100/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1139 - f1_score: 0.5721 - categorical_accuracy: 0.7511 - val_loss: 0.3211 - val_f1_score: 0.3441 - val_categorical_accuracy: 0.6237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3c4c15d1e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.19e-3\n",
    "EPOCHS=100\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.3, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 1s 2ms/step\n",
      "Training\n",
      "[[ 253  313   48]\n",
      " [ 200 3089  112]\n",
      " [  54  442  179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.41      0.45       614\n",
      "         1.0       0.80      0.91      0.85      3401\n",
      "         2.0       0.53      0.27      0.35       675\n",
      "\n",
      "    accuracy                           0.75      4690\n",
      "   macro avg       0.61      0.53      0.55      4690\n",
      "weighted avg       0.72      0.75      0.73      4690\n",
      "\n",
      "21/21 [==============================] - 0s 2ms/step\n",
      "Testing\n",
      "[[ 14  68   6]\n",
      " [ 54 394  38]\n",
      " [ 12  73  11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.16      0.17        88\n",
      "         1.0       0.74      0.81      0.77       486\n",
      "         2.0       0.20      0.11      0.15        96\n",
      "\n",
      "    accuracy                           0.63       670\n",
      "   macro avg       0.37      0.36      0.36       670\n",
      "weighted avg       0.59      0.63      0.60       670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(1)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d_12 (Separ  (None, 13, 32)            1782      \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 13, 32)            128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " separable_conv1d_13 (Separ  (None, 11, 16)            624       \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 11, 16)            64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " separable_conv1d_14 (Separ  (None, 9, 8)              184       \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 9, 8)              32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "226/226 [==============================] - 5s 7ms/step - loss: 0.1221 - f1_score: 0.3235 - categorical_accuracy: 0.6080 - val_loss: 0.0978 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 2/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0996 - f1_score: 0.2781 - categorical_accuracy: 0.7157 - val_loss: 0.0975 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 3/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0977 - f1_score: 0.2783 - categorical_accuracy: 0.7162 - val_loss: 0.0978 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 4/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0971 - f1_score: 0.2782 - categorical_accuracy: 0.7162 - val_loss: 0.0982 - val_f1_score: 0.2820 - val_categorical_accuracy: 0.7331\n",
      "Epoch 5/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0960 - f1_score: 0.2780 - categorical_accuracy: 0.7154 - val_loss: 0.0980 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7342\n",
      "Epoch 6/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0955 - f1_score: 0.2805 - categorical_accuracy: 0.7159 - val_loss: 0.0984 - val_f1_score: 0.2820 - val_categorical_accuracy: 0.7331\n",
      "Epoch 7/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0940 - f1_score: 0.2860 - categorical_accuracy: 0.7176 - val_loss: 0.0996 - val_f1_score: 0.2820 - val_categorical_accuracy: 0.7331\n",
      "Epoch 8/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0938 - f1_score: 0.2841 - categorical_accuracy: 0.7168 - val_loss: 0.0993 - val_f1_score: 0.2820 - val_categorical_accuracy: 0.7331\n",
      "Epoch 9/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0929 - f1_score: 0.2860 - categorical_accuracy: 0.7151 - val_loss: 0.0995 - val_f1_score: 0.2820 - val_categorical_accuracy: 0.7331\n",
      "Epoch 10/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0923 - f1_score: 0.2887 - categorical_accuracy: 0.7148 - val_loss: 0.1004 - val_f1_score: 0.2815 - val_categorical_accuracy: 0.7309\n",
      "Epoch 11/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0915 - f1_score: 0.2960 - categorical_accuracy: 0.7173 - val_loss: 0.1010 - val_f1_score: 0.2817 - val_categorical_accuracy: 0.7309\n",
      "Epoch 12/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0911 - f1_score: 0.3029 - categorical_accuracy: 0.7179 - val_loss: 0.1025 - val_f1_score: 0.2809 - val_categorical_accuracy: 0.7276\n",
      "Epoch 13/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0907 - f1_score: 0.3112 - categorical_accuracy: 0.7157 - val_loss: 0.1028 - val_f1_score: 0.2862 - val_categorical_accuracy: 0.7265\n",
      "Epoch 14/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0894 - f1_score: 0.3162 - categorical_accuracy: 0.7201 - val_loss: 0.1024 - val_f1_score: 0.2848 - val_categorical_accuracy: 0.7220\n",
      "Epoch 15/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0893 - f1_score: 0.3227 - categorical_accuracy: 0.7184 - val_loss: 0.1050 - val_f1_score: 0.2894 - val_categorical_accuracy: 0.7198\n",
      "Epoch 16/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0887 - f1_score: 0.3179 - categorical_accuracy: 0.7179 - val_loss: 0.1030 - val_f1_score: 0.2909 - val_categorical_accuracy: 0.7254\n",
      "Epoch 17/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0879 - f1_score: 0.3262 - categorical_accuracy: 0.7173 - val_loss: 0.1034 - val_f1_score: 0.2864 - val_categorical_accuracy: 0.7276\n",
      "Epoch 18/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0869 - f1_score: 0.3311 - categorical_accuracy: 0.7176 - val_loss: 0.1046 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7110\n",
      "Epoch 19/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0868 - f1_score: 0.3460 - categorical_accuracy: 0.7212 - val_loss: 0.1049 - val_f1_score: 0.2928 - val_categorical_accuracy: 0.7143\n",
      "Epoch 20/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0861 - f1_score: 0.3464 - categorical_accuracy: 0.7204 - val_loss: 0.1059 - val_f1_score: 0.3048 - val_categorical_accuracy: 0.7065\n",
      "Epoch 21/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0853 - f1_score: 0.3572 - categorical_accuracy: 0.7173 - val_loss: 0.1061 - val_f1_score: 0.3044 - val_categorical_accuracy: 0.7198\n",
      "Epoch 22/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0845 - f1_score: 0.3750 - categorical_accuracy: 0.7218 - val_loss: 0.1075 - val_f1_score: 0.3005 - val_categorical_accuracy: 0.7065\n",
      "Epoch 23/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0843 - f1_score: 0.3824 - categorical_accuracy: 0.7237 - val_loss: 0.1070 - val_f1_score: 0.2992 - val_categorical_accuracy: 0.7021\n",
      "Epoch 24/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0840 - f1_score: 0.3706 - categorical_accuracy: 0.7215 - val_loss: 0.1083 - val_f1_score: 0.2955 - val_categorical_accuracy: 0.7065\n",
      "Epoch 25/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0827 - f1_score: 0.3872 - categorical_accuracy: 0.7240 - val_loss: 0.1087 - val_f1_score: 0.2996 - val_categorical_accuracy: 0.7032\n",
      "Epoch 26/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0830 - f1_score: 0.3842 - categorical_accuracy: 0.7218 - val_loss: 0.1108 - val_f1_score: 0.2922 - val_categorical_accuracy: 0.6955\n",
      "Epoch 27/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0812 - f1_score: 0.3920 - categorical_accuracy: 0.7198 - val_loss: 0.1107 - val_f1_score: 0.2989 - val_categorical_accuracy: 0.6866\n",
      "Epoch 28/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0812 - f1_score: 0.4231 - categorical_accuracy: 0.7259 - val_loss: 0.1116 - val_f1_score: 0.3113 - val_categorical_accuracy: 0.6988\n",
      "Epoch 29/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0804 - f1_score: 0.3995 - categorical_accuracy: 0.7212 - val_loss: 0.1136 - val_f1_score: 0.2890 - val_categorical_accuracy: 0.6678\n",
      "Epoch 30/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0795 - f1_score: 0.4288 - categorical_accuracy: 0.7267 - val_loss: 0.1142 - val_f1_score: 0.2950 - val_categorical_accuracy: 0.6877\n",
      "Epoch 31/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0795 - f1_score: 0.4295 - categorical_accuracy: 0.7273 - val_loss: 0.1128 - val_f1_score: 0.3031 - val_categorical_accuracy: 0.6877\n",
      "Epoch 32/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0795 - f1_score: 0.4310 - categorical_accuracy: 0.7248 - val_loss: 0.1153 - val_f1_score: 0.3058 - val_categorical_accuracy: 0.6833\n",
      "Epoch 33/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0788 - f1_score: 0.4363 - categorical_accuracy: 0.7273 - val_loss: 0.1156 - val_f1_score: 0.2957 - val_categorical_accuracy: 0.6766\n",
      "Epoch 34/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0783 - f1_score: 0.4298 - categorical_accuracy: 0.7234 - val_loss: 0.1174 - val_f1_score: 0.3001 - val_categorical_accuracy: 0.6645\n",
      "Epoch 35/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0771 - f1_score: 0.4514 - categorical_accuracy: 0.7309 - val_loss: 0.1186 - val_f1_score: 0.3134 - val_categorical_accuracy: 0.6611\n",
      "Epoch 36/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0766 - f1_score: 0.4587 - categorical_accuracy: 0.7306 - val_loss: 0.1161 - val_f1_score: 0.3251 - val_categorical_accuracy: 0.6656\n",
      "Epoch 37/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0768 - f1_score: 0.4666 - categorical_accuracy: 0.7342 - val_loss: 0.1207 - val_f1_score: 0.3175 - val_categorical_accuracy: 0.6545\n",
      "Epoch 38/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0762 - f1_score: 0.4642 - categorical_accuracy: 0.7292 - val_loss: 0.1219 - val_f1_score: 0.3022 - val_categorical_accuracy: 0.6600\n",
      "Epoch 39/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0751 - f1_score: 0.4774 - categorical_accuracy: 0.7345 - val_loss: 0.1220 - val_f1_score: 0.3119 - val_categorical_accuracy: 0.6478\n",
      "Epoch 40/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0750 - f1_score: 0.4779 - categorical_accuracy: 0.7303 - val_loss: 0.1199 - val_f1_score: 0.3217 - val_categorical_accuracy: 0.6489\n",
      "Epoch 41/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0749 - f1_score: 0.4659 - categorical_accuracy: 0.7251 - val_loss: 0.1204 - val_f1_score: 0.3117 - val_categorical_accuracy: 0.6567\n",
      "Epoch 42/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0742 - f1_score: 0.4797 - categorical_accuracy: 0.7301 - val_loss: 0.1210 - val_f1_score: 0.3034 - val_categorical_accuracy: 0.6434\n",
      "Epoch 43/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0741 - f1_score: 0.4778 - categorical_accuracy: 0.7301 - val_loss: 0.1244 - val_f1_score: 0.3077 - val_categorical_accuracy: 0.6368\n",
      "Epoch 44/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0736 - f1_score: 0.4943 - categorical_accuracy: 0.7348 - val_loss: 0.1252 - val_f1_score: 0.3162 - val_categorical_accuracy: 0.6523\n",
      "Epoch 45/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0734 - f1_score: 0.4885 - categorical_accuracy: 0.7306 - val_loss: 0.1253 - val_f1_score: 0.3158 - val_categorical_accuracy: 0.6523\n",
      "Epoch 46/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0729 - f1_score: 0.4881 - categorical_accuracy: 0.7309 - val_loss: 0.1259 - val_f1_score: 0.3181 - val_categorical_accuracy: 0.6478\n",
      "Epoch 47/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0723 - f1_score: 0.5040 - categorical_accuracy: 0.7359 - val_loss: 0.1260 - val_f1_score: 0.3153 - val_categorical_accuracy: 0.6489\n",
      "Epoch 48/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0724 - f1_score: 0.4815 - categorical_accuracy: 0.7267 - val_loss: 0.1279 - val_f1_score: 0.3092 - val_categorical_accuracy: 0.6235\n",
      "Epoch 49/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0712 - f1_score: 0.5136 - categorical_accuracy: 0.7386 - val_loss: 0.1265 - val_f1_score: 0.3090 - val_categorical_accuracy: 0.6501\n",
      "Epoch 50/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0715 - f1_score: 0.4975 - categorical_accuracy: 0.7298 - val_loss: 0.1270 - val_f1_score: 0.3179 - val_categorical_accuracy: 0.6467\n",
      "Epoch 51/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0717 - f1_score: 0.5111 - categorical_accuracy: 0.7381 - val_loss: 0.1324 - val_f1_score: 0.3332 - val_categorical_accuracy: 0.6456\n",
      "Epoch 52/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0707 - f1_score: 0.5080 - categorical_accuracy: 0.7350 - val_loss: 0.1294 - val_f1_score: 0.3440 - val_categorical_accuracy: 0.6423\n",
      "Epoch 53/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0716 - f1_score: 0.4907 - categorical_accuracy: 0.7267 - val_loss: 0.1306 - val_f1_score: 0.3386 - val_categorical_accuracy: 0.6290\n",
      "Epoch 54/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0702 - f1_score: 0.5041 - categorical_accuracy: 0.7276 - val_loss: 0.1306 - val_f1_score: 0.3186 - val_categorical_accuracy: 0.6478\n",
      "Epoch 55/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0702 - f1_score: 0.5172 - categorical_accuracy: 0.7342 - val_loss: 0.1321 - val_f1_score: 0.3286 - val_categorical_accuracy: 0.6357\n",
      "Epoch 56/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0701 - f1_score: 0.5052 - categorical_accuracy: 0.7339 - val_loss: 0.1294 - val_f1_score: 0.3254 - val_categorical_accuracy: 0.6412\n",
      "Epoch 57/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0693 - f1_score: 0.5158 - categorical_accuracy: 0.7350 - val_loss: 0.1304 - val_f1_score: 0.3391 - val_categorical_accuracy: 0.6312\n",
      "Epoch 58/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0689 - f1_score: 0.5237 - categorical_accuracy: 0.7367 - val_loss: 0.1328 - val_f1_score: 0.3425 - val_categorical_accuracy: 0.6390\n",
      "Epoch 59/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0690 - f1_score: 0.5126 - categorical_accuracy: 0.7290 - val_loss: 0.1321 - val_f1_score: 0.3298 - val_categorical_accuracy: 0.6290\n",
      "Epoch 60/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0687 - f1_score: 0.5189 - categorical_accuracy: 0.7345 - val_loss: 0.1333 - val_f1_score: 0.3320 - val_categorical_accuracy: 0.6213\n",
      "Epoch 61/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0690 - f1_score: 0.5199 - categorical_accuracy: 0.7301 - val_loss: 0.1316 - val_f1_score: 0.3472 - val_categorical_accuracy: 0.6556\n",
      "Epoch 62/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0686 - f1_score: 0.5236 - categorical_accuracy: 0.7386 - val_loss: 0.1338 - val_f1_score: 0.3342 - val_categorical_accuracy: 0.6346\n",
      "Epoch 63/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0688 - f1_score: 0.5120 - categorical_accuracy: 0.7284 - val_loss: 0.1358 - val_f1_score: 0.3454 - val_categorical_accuracy: 0.6213\n",
      "Epoch 64/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0683 - f1_score: 0.5261 - categorical_accuracy: 0.7353 - val_loss: 0.1346 - val_f1_score: 0.3495 - val_categorical_accuracy: 0.6312\n",
      "Epoch 65/85\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 0.0680 - f1_score: 0.5360 - categorical_accuracy: 0.7381 - val_loss: 0.1340 - val_f1_score: 0.3359 - val_categorical_accuracy: 0.6246\n",
      "Epoch 66/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0673 - f1_score: 0.5421 - categorical_accuracy: 0.7436 - val_loss: 0.1367 - val_f1_score: 0.3262 - val_categorical_accuracy: 0.6091\n",
      "Epoch 67/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0673 - f1_score: 0.5487 - categorical_accuracy: 0.7442 - val_loss: 0.1351 - val_f1_score: 0.3266 - val_categorical_accuracy: 0.6368\n",
      "Epoch 68/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0681 - f1_score: 0.5129 - categorical_accuracy: 0.7312 - val_loss: 0.1363 - val_f1_score: 0.3336 - val_categorical_accuracy: 0.6058\n",
      "Epoch 69/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0668 - f1_score: 0.5330 - categorical_accuracy: 0.7375 - val_loss: 0.1386 - val_f1_score: 0.3140 - val_categorical_accuracy: 0.6202\n",
      "Epoch 70/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0671 - f1_score: 0.5203 - categorical_accuracy: 0.7287 - val_loss: 0.1399 - val_f1_score: 0.3162 - val_categorical_accuracy: 0.6257\n",
      "Epoch 71/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0669 - f1_score: 0.5257 - categorical_accuracy: 0.7328 - val_loss: 0.1379 - val_f1_score: 0.3136 - val_categorical_accuracy: 0.6268\n",
      "Epoch 72/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0662 - f1_score: 0.5252 - categorical_accuracy: 0.7350 - val_loss: 0.1382 - val_f1_score: 0.3146 - val_categorical_accuracy: 0.6224\n",
      "Epoch 73/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0663 - f1_score: 0.5381 - categorical_accuracy: 0.7434 - val_loss: 0.1401 - val_f1_score: 0.3200 - val_categorical_accuracy: 0.5936\n",
      "Epoch 74/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0663 - f1_score: 0.5339 - categorical_accuracy: 0.7364 - val_loss: 0.1421 - val_f1_score: 0.3345 - val_categorical_accuracy: 0.6146\n",
      "Epoch 75/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0661 - f1_score: 0.5346 - categorical_accuracy: 0.7326 - val_loss: 0.1391 - val_f1_score: 0.3171 - val_categorical_accuracy: 0.6124\n",
      "Epoch 76/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0662 - f1_score: 0.5246 - categorical_accuracy: 0.7295 - val_loss: 0.1417 - val_f1_score: 0.3326 - val_categorical_accuracy: 0.6157\n",
      "Epoch 77/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0649 - f1_score: 0.5437 - categorical_accuracy: 0.7398 - val_loss: 0.1414 - val_f1_score: 0.3472 - val_categorical_accuracy: 0.6113\n",
      "Epoch 78/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0657 - f1_score: 0.5532 - categorical_accuracy: 0.7386 - val_loss: 0.1433 - val_f1_score: 0.3399 - val_categorical_accuracy: 0.6334\n",
      "Epoch 79/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0659 - f1_score: 0.5373 - categorical_accuracy: 0.7373 - val_loss: 0.1418 - val_f1_score: 0.3240 - val_categorical_accuracy: 0.5980\n",
      "Epoch 80/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0652 - f1_score: 0.5453 - categorical_accuracy: 0.7389 - val_loss: 0.1424 - val_f1_score: 0.3341 - val_categorical_accuracy: 0.6058\n",
      "Epoch 81/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0656 - f1_score: 0.5368 - categorical_accuracy: 0.7345 - val_loss: 0.1426 - val_f1_score: 0.3440 - val_categorical_accuracy: 0.6058\n",
      "Epoch 82/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0652 - f1_score: 0.5433 - categorical_accuracy: 0.7389 - val_loss: 0.1445 - val_f1_score: 0.3378 - val_categorical_accuracy: 0.5991\n",
      "Epoch 83/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0646 - f1_score: 0.5416 - categorical_accuracy: 0.7406 - val_loss: 0.1422 - val_f1_score: 0.3353 - val_categorical_accuracy: 0.6102\n",
      "Epoch 84/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0648 - f1_score: 0.5457 - categorical_accuracy: 0.7422 - val_loss: 0.1433 - val_f1_score: 0.3329 - val_categorical_accuracy: 0.5958\n",
      "Epoch 85/85\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 0.0645 - f1_score: 0.5465 - categorical_accuracy: 0.7364 - val_loss: 0.1418 - val_f1_score: 0.3341 - val_categorical_accuracy: 0.5936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3c94bcdb70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.2e-3\n",
    "EPOCHS=85\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.15, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 1s 2ms/step\n",
      "Training\n",
      "[[ 204  359   27]\n",
      " [ 163 2870  217]\n",
      " [  27  419  229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.35      0.41       590\n",
      "         1.0       0.79      0.88      0.83      3250\n",
      "         2.0       0.48      0.34      0.40       675\n",
      "\n",
      "    accuracy                           0.73      4515\n",
      "   macro avg       0.60      0.52      0.55      4515\n",
      "weighted avg       0.71      0.73      0.71      4515\n",
      "\n",
      "21/21 [==============================] - 0s 3ms/step\n",
      "Testing\n",
      "[[  6  70   8]\n",
      " [ 47 369  49]\n",
      " [  6  81   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.10      0.07      0.08        84\n",
      "         1.0       0.71      0.79      0.75       465\n",
      "         2.0       0.14      0.09      0.11        96\n",
      "\n",
      "    accuracy                           0.60       645\n",
      "   macro avg       0.32      0.32      0.31       645\n",
      "weighted avg       0.55      0.60      0.57       645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Pentad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pentad_data(2)\n",
    "\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 15, 50)]          0         \n",
      "                                                                 \n",
      " separable_conv1d_9 (Separa  (None, 13, 32)            1782      \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 13, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv1d_10 (Separ  (None, 11, 16)            624       \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 11, 16)            64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " separable_conv1d_11 (Separ  (None, 9, 8)              184       \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 9, 8)              32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                730       \n",
      "                                                                 \n",
      " logits (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3577 (13.97 KB)\n",
      "Trainable params: 3465 (13.54 KB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/85\n",
      "217/217 [==============================] - 5s 7ms/step - loss: 0.1213 - f1_score: 0.3217 - categorical_accuracy: 0.6143 - val_loss: 0.1019 - val_f1_score: 0.2770 - val_categorical_accuracy: 0.7108\n",
      "Epoch 2/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.1017 - f1_score: 0.2799 - categorical_accuracy: 0.7126 - val_loss: 0.1011 - val_f1_score: 0.2770 - val_categorical_accuracy: 0.7108\n",
      "Epoch 3/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0991 - f1_score: 0.2776 - categorical_accuracy: 0.7134 - val_loss: 0.1015 - val_f1_score: 0.2770 - val_categorical_accuracy: 0.7108\n",
      "Epoch 4/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0982 - f1_score: 0.2777 - categorical_accuracy: 0.7140 - val_loss: 0.1013 - val_f1_score: 0.2770 - val_categorical_accuracy: 0.7108\n",
      "Epoch 5/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0970 - f1_score: 0.2789 - categorical_accuracy: 0.7140 - val_loss: 0.1017 - val_f1_score: 0.2830 - val_categorical_accuracy: 0.7120\n",
      "Epoch 6/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0963 - f1_score: 0.2776 - categorical_accuracy: 0.7134 - val_loss: 0.1021 - val_f1_score: 0.2769 - val_categorical_accuracy: 0.7097\n",
      "Epoch 7/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0955 - f1_score: 0.2827 - categorical_accuracy: 0.7143 - val_loss: 0.1021 - val_f1_score: 0.2770 - val_categorical_accuracy: 0.7108\n",
      "Epoch 8/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0946 - f1_score: 0.2852 - categorical_accuracy: 0.7137 - val_loss: 0.1030 - val_f1_score: 0.2828 - val_categorical_accuracy: 0.7108\n",
      "Epoch 9/85\n",
      "217/217 [==============================] - 1s 5ms/step - loss: 0.0941 - f1_score: 0.2929 - categorical_accuracy: 0.7166 - val_loss: 0.1039 - val_f1_score: 0.2832 - val_categorical_accuracy: 0.7120\n",
      "Epoch 10/85\n",
      "217/217 [==============================] - 1s 5ms/step - loss: 0.0930 - f1_score: 0.2871 - categorical_accuracy: 0.7143 - val_loss: 0.1039 - val_f1_score: 0.2828 - val_categorical_accuracy: 0.7108\n",
      "Epoch 11/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0925 - f1_score: 0.2934 - categorical_accuracy: 0.7157 - val_loss: 0.1047 - val_f1_score: 0.2822 - val_categorical_accuracy: 0.7085\n",
      "Epoch 12/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0915 - f1_score: 0.3062 - categorical_accuracy: 0.7154 - val_loss: 0.1057 - val_f1_score: 0.2819 - val_categorical_accuracy: 0.7108\n",
      "Epoch 13/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0907 - f1_score: 0.3136 - categorical_accuracy: 0.7166 - val_loss: 0.1066 - val_f1_score: 0.2858 - val_categorical_accuracy: 0.7028\n",
      "Epoch 14/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0900 - f1_score: 0.3109 - categorical_accuracy: 0.7157 - val_loss: 0.1080 - val_f1_score: 0.2984 - val_categorical_accuracy: 0.7131\n",
      "Epoch 15/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0894 - f1_score: 0.3176 - categorical_accuracy: 0.7157 - val_loss: 0.1092 - val_f1_score: 0.2907 - val_categorical_accuracy: 0.7051\n",
      "Epoch 16/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0884 - f1_score: 0.3217 - categorical_accuracy: 0.7157 - val_loss: 0.1101 - val_f1_score: 0.3044 - val_categorical_accuracy: 0.7051\n",
      "Epoch 17/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0877 - f1_score: 0.3370 - categorical_accuracy: 0.7186 - val_loss: 0.1100 - val_f1_score: 0.2952 - val_categorical_accuracy: 0.7062\n",
      "Epoch 18/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0869 - f1_score: 0.3524 - categorical_accuracy: 0.7209 - val_loss: 0.1108 - val_f1_score: 0.3087 - val_categorical_accuracy: 0.7016\n",
      "Epoch 19/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0863 - f1_score: 0.3568 - categorical_accuracy: 0.7186 - val_loss: 0.1117 - val_f1_score: 0.3000 - val_categorical_accuracy: 0.7005\n",
      "Epoch 20/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0854 - f1_score: 0.3678 - categorical_accuracy: 0.7195 - val_loss: 0.1132 - val_f1_score: 0.3167 - val_categorical_accuracy: 0.6970\n",
      "Epoch 21/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0855 - f1_score: 0.3617 - categorical_accuracy: 0.7192 - val_loss: 0.1126 - val_f1_score: 0.3071 - val_categorical_accuracy: 0.6959\n",
      "Epoch 22/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0844 - f1_score: 0.3781 - categorical_accuracy: 0.7192 - val_loss: 0.1142 - val_f1_score: 0.3049 - val_categorical_accuracy: 0.6901\n",
      "Epoch 23/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0835 - f1_score: 0.3818 - categorical_accuracy: 0.7189 - val_loss: 0.1152 - val_f1_score: 0.3057 - val_categorical_accuracy: 0.6924\n",
      "Epoch 24/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0831 - f1_score: 0.3915 - categorical_accuracy: 0.7224 - val_loss: 0.1167 - val_f1_score: 0.3155 - val_categorical_accuracy: 0.6935\n",
      "Epoch 25/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0822 - f1_score: 0.3898 - categorical_accuracy: 0.7189 - val_loss: 0.1165 - val_f1_score: 0.3081 - val_categorical_accuracy: 0.6763\n",
      "Epoch 26/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0824 - f1_score: 0.4096 - categorical_accuracy: 0.7235 - val_loss: 0.1170 - val_f1_score: 0.3103 - val_categorical_accuracy: 0.6832\n",
      "Epoch 27/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0815 - f1_score: 0.4088 - categorical_accuracy: 0.7224 - val_loss: 0.1178 - val_f1_score: 0.3170 - val_categorical_accuracy: 0.6705\n",
      "Epoch 28/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0804 - f1_score: 0.4232 - categorical_accuracy: 0.7261 - val_loss: 0.1193 - val_f1_score: 0.3108 - val_categorical_accuracy: 0.6751\n",
      "Epoch 29/85\n",
      "217/217 [==============================] - 1s 5ms/step - loss: 0.0803 - f1_score: 0.4238 - categorical_accuracy: 0.7258 - val_loss: 0.1208 - val_f1_score: 0.3182 - val_categorical_accuracy: 0.6705\n",
      "Epoch 30/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0793 - f1_score: 0.4338 - categorical_accuracy: 0.7275 - val_loss: 0.1207 - val_f1_score: 0.3194 - val_categorical_accuracy: 0.6740\n",
      "Epoch 31/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0786 - f1_score: 0.4345 - categorical_accuracy: 0.7252 - val_loss: 0.1212 - val_f1_score: 0.3387 - val_categorical_accuracy: 0.6694\n",
      "Epoch 32/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0784 - f1_score: 0.4268 - categorical_accuracy: 0.7244 - val_loss: 0.1230 - val_f1_score: 0.3207 - val_categorical_accuracy: 0.6601\n",
      "Epoch 33/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0776 - f1_score: 0.4576 - categorical_accuracy: 0.7275 - val_loss: 0.1240 - val_f1_score: 0.3238 - val_categorical_accuracy: 0.6763\n",
      "Epoch 34/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0770 - f1_score: 0.4437 - categorical_accuracy: 0.7249 - val_loss: 0.1255 - val_f1_score: 0.3133 - val_categorical_accuracy: 0.6601\n",
      "Epoch 35/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0765 - f1_score: 0.4628 - categorical_accuracy: 0.7359 - val_loss: 0.1256 - val_f1_score: 0.3415 - val_categorical_accuracy: 0.6682\n",
      "Epoch 36/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0758 - f1_score: 0.4689 - categorical_accuracy: 0.7296 - val_loss: 0.1248 - val_f1_score: 0.3306 - val_categorical_accuracy: 0.6671\n",
      "Epoch 37/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0751 - f1_score: 0.4614 - categorical_accuracy: 0.7252 - val_loss: 0.1271 - val_f1_score: 0.3257 - val_categorical_accuracy: 0.6578\n",
      "Epoch 38/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0751 - f1_score: 0.4763 - categorical_accuracy: 0.7330 - val_loss: 0.1291 - val_f1_score: 0.3260 - val_categorical_accuracy: 0.6555\n",
      "Epoch 39/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0742 - f1_score: 0.4733 - categorical_accuracy: 0.7333 - val_loss: 0.1304 - val_f1_score: 0.3274 - val_categorical_accuracy: 0.6590\n",
      "Epoch 40/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0742 - f1_score: 0.4817 - categorical_accuracy: 0.7319 - val_loss: 0.1285 - val_f1_score: 0.3114 - val_categorical_accuracy: 0.6463\n",
      "Epoch 41/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0727 - f1_score: 0.4979 - categorical_accuracy: 0.7368 - val_loss: 0.1326 - val_f1_score: 0.3391 - val_categorical_accuracy: 0.6509\n",
      "Epoch 42/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0734 - f1_score: 0.4883 - categorical_accuracy: 0.7342 - val_loss: 0.1318 - val_f1_score: 0.3458 - val_categorical_accuracy: 0.6682\n",
      "Epoch 43/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0726 - f1_score: 0.4809 - categorical_accuracy: 0.7287 - val_loss: 0.1323 - val_f1_score: 0.3493 - val_categorical_accuracy: 0.6429\n",
      "Epoch 44/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0728 - f1_score: 0.4887 - categorical_accuracy: 0.7296 - val_loss: 0.1328 - val_f1_score: 0.3285 - val_categorical_accuracy: 0.6452\n",
      "Epoch 45/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0713 - f1_score: 0.4970 - categorical_accuracy: 0.7356 - val_loss: 0.1351 - val_f1_score: 0.3201 - val_categorical_accuracy: 0.6509\n",
      "Epoch 46/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0713 - f1_score: 0.4897 - categorical_accuracy: 0.7336 - val_loss: 0.1365 - val_f1_score: 0.3431 - val_categorical_accuracy: 0.6429\n",
      "Epoch 47/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0702 - f1_score: 0.5035 - categorical_accuracy: 0.7365 - val_loss: 0.1364 - val_f1_score: 0.3320 - val_categorical_accuracy: 0.6406\n",
      "Epoch 48/85\n",
      "217/217 [==============================] - 1s 5ms/step - loss: 0.0709 - f1_score: 0.5113 - categorical_accuracy: 0.7347 - val_loss: 0.1400 - val_f1_score: 0.3321 - val_categorical_accuracy: 0.6509\n",
      "Epoch 49/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0699 - f1_score: 0.5053 - categorical_accuracy: 0.7385 - val_loss: 0.1383 - val_f1_score: 0.3428 - val_categorical_accuracy: 0.6394\n",
      "Epoch 50/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0697 - f1_score: 0.5165 - categorical_accuracy: 0.7396 - val_loss: 0.1415 - val_f1_score: 0.3309 - val_categorical_accuracy: 0.6406\n",
      "Epoch 51/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0692 - f1_score: 0.5167 - categorical_accuracy: 0.7373 - val_loss: 0.1418 - val_f1_score: 0.3330 - val_categorical_accuracy: 0.6336\n",
      "Epoch 52/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0685 - f1_score: 0.5206 - categorical_accuracy: 0.7396 - val_loss: 0.1414 - val_f1_score: 0.3353 - val_categorical_accuracy: 0.6221\n",
      "Epoch 53/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0685 - f1_score: 0.5261 - categorical_accuracy: 0.7414 - val_loss: 0.1434 - val_f1_score: 0.3303 - val_categorical_accuracy: 0.6382\n",
      "Epoch 54/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0676 - f1_score: 0.5226 - categorical_accuracy: 0.7440 - val_loss: 0.1428 - val_f1_score: 0.3358 - val_categorical_accuracy: 0.6210\n",
      "Epoch 55/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0682 - f1_score: 0.5153 - categorical_accuracy: 0.7333 - val_loss: 0.1433 - val_f1_score: 0.3347 - val_categorical_accuracy: 0.6382\n",
      "Epoch 56/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0675 - f1_score: 0.5244 - categorical_accuracy: 0.7425 - val_loss: 0.1427 - val_f1_score: 0.3393 - val_categorical_accuracy: 0.6348\n",
      "Epoch 57/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0674 - f1_score: 0.5210 - categorical_accuracy: 0.7350 - val_loss: 0.1454 - val_f1_score: 0.3314 - val_categorical_accuracy: 0.6187\n",
      "Epoch 58/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0663 - f1_score: 0.5366 - categorical_accuracy: 0.7402 - val_loss: 0.1476 - val_f1_score: 0.3331 - val_categorical_accuracy: 0.6118\n",
      "Epoch 59/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0659 - f1_score: 0.5374 - categorical_accuracy: 0.7379 - val_loss: 0.1475 - val_f1_score: 0.3382 - val_categorical_accuracy: 0.6233\n",
      "Epoch 60/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0666 - f1_score: 0.5347 - categorical_accuracy: 0.7344 - val_loss: 0.1484 - val_f1_score: 0.3442 - val_categorical_accuracy: 0.6198\n",
      "Epoch 61/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0660 - f1_score: 0.5305 - categorical_accuracy: 0.7379 - val_loss: 0.1494 - val_f1_score: 0.3297 - val_categorical_accuracy: 0.5991\n",
      "Epoch 62/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0655 - f1_score: 0.5396 - categorical_accuracy: 0.7391 - val_loss: 0.1511 - val_f1_score: 0.3301 - val_categorical_accuracy: 0.6083\n",
      "Epoch 63/85\n",
      "217/217 [==============================] - 1s 5ms/step - loss: 0.0663 - f1_score: 0.5358 - categorical_accuracy: 0.7344 - val_loss: 0.1494 - val_f1_score: 0.3499 - val_categorical_accuracy: 0.6348\n",
      "Epoch 64/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0648 - f1_score: 0.5296 - categorical_accuracy: 0.7365 - val_loss: 0.1492 - val_f1_score: 0.3298 - val_categorical_accuracy: 0.6071\n",
      "Epoch 65/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0652 - f1_score: 0.5371 - categorical_accuracy: 0.7376 - val_loss: 0.1518 - val_f1_score: 0.3334 - val_categorical_accuracy: 0.6025\n",
      "Epoch 66/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0656 - f1_score: 0.5458 - categorical_accuracy: 0.7353 - val_loss: 0.1516 - val_f1_score: 0.3437 - val_categorical_accuracy: 0.6279\n",
      "Epoch 67/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0652 - f1_score: 0.5418 - categorical_accuracy: 0.7405 - val_loss: 0.1512 - val_f1_score: 0.3243 - val_categorical_accuracy: 0.6014\n",
      "Epoch 68/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0645 - f1_score: 0.5464 - categorical_accuracy: 0.7353 - val_loss: 0.1511 - val_f1_score: 0.3510 - val_categorical_accuracy: 0.6129\n",
      "Epoch 69/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0645 - f1_score: 0.5440 - categorical_accuracy: 0.7399 - val_loss: 0.1505 - val_f1_score: 0.3347 - val_categorical_accuracy: 0.6129\n",
      "Epoch 70/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0642 - f1_score: 0.5347 - categorical_accuracy: 0.7324 - val_loss: 0.1515 - val_f1_score: 0.3445 - val_categorical_accuracy: 0.6083\n",
      "Epoch 71/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0635 - f1_score: 0.5533 - categorical_accuracy: 0.7416 - val_loss: 0.1544 - val_f1_score: 0.3274 - val_categorical_accuracy: 0.6037\n",
      "Epoch 72/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0640 - f1_score: 0.5459 - categorical_accuracy: 0.7382 - val_loss: 0.1553 - val_f1_score: 0.3488 - val_categorical_accuracy: 0.6359\n",
      "Epoch 73/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0638 - f1_score: 0.5315 - categorical_accuracy: 0.7350 - val_loss: 0.1546 - val_f1_score: 0.3437 - val_categorical_accuracy: 0.6060\n",
      "Epoch 74/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0628 - f1_score: 0.5475 - categorical_accuracy: 0.7336 - val_loss: 0.1571 - val_f1_score: 0.3314 - val_categorical_accuracy: 0.5864\n",
      "Epoch 75/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0633 - f1_score: 0.5495 - categorical_accuracy: 0.7376 - val_loss: 0.1561 - val_f1_score: 0.3340 - val_categorical_accuracy: 0.5853\n",
      "Epoch 76/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0631 - f1_score: 0.5574 - categorical_accuracy: 0.7411 - val_loss: 0.1563 - val_f1_score: 0.3349 - val_categorical_accuracy: 0.5818\n",
      "Epoch 77/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0634 - f1_score: 0.5482 - categorical_accuracy: 0.7310 - val_loss: 0.1558 - val_f1_score: 0.3607 - val_categorical_accuracy: 0.6244\n",
      "Epoch 78/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0621 - f1_score: 0.5637 - categorical_accuracy: 0.7422 - val_loss: 0.1559 - val_f1_score: 0.3396 - val_categorical_accuracy: 0.6071\n",
      "Epoch 79/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0628 - f1_score: 0.5461 - categorical_accuracy: 0.7391 - val_loss: 0.1564 - val_f1_score: 0.3389 - val_categorical_accuracy: 0.6002\n",
      "Epoch 80/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0631 - f1_score: 0.5583 - categorical_accuracy: 0.7402 - val_loss: 0.1568 - val_f1_score: 0.3454 - val_categorical_accuracy: 0.6129\n",
      "Epoch 81/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0623 - f1_score: 0.5544 - categorical_accuracy: 0.7416 - val_loss: 0.1588 - val_f1_score: 0.3420 - val_categorical_accuracy: 0.6118\n",
      "Epoch 82/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0616 - f1_score: 0.5578 - categorical_accuracy: 0.7419 - val_loss: 0.1601 - val_f1_score: 0.3437 - val_categorical_accuracy: 0.6083\n",
      "Epoch 83/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0622 - f1_score: 0.5582 - categorical_accuracy: 0.7414 - val_loss: 0.1580 - val_f1_score: 0.3435 - val_categorical_accuracy: 0.6164\n",
      "Epoch 84/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0620 - f1_score: 0.5632 - categorical_accuracy: 0.7408 - val_loss: 0.1593 - val_f1_score: 0.3409 - val_categorical_accuracy: 0.6071\n",
      "Epoch 85/85\n",
      "217/217 [==============================] - 1s 4ms/step - loss: 0.0623 - f1_score: 0.5566 - categorical_accuracy: 0.7402 - val_loss: 0.1611 - val_f1_score: 0.3368 - val_categorical_accuracy: 0.6083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3c94bcc3a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(15, 50))\n",
    "x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5)\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                           activation=keras.layers.ELU(),\n",
    "                           depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           pointwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                           depthwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           pointwise_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "                           )(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(10, activation=keras.layers.LeakyReLU())(x)\n",
    "x = keras.layers.Dense(3, name='logits')(x)\n",
    "output = layers.Activation('softmax')(x)\n",
    "\n",
    "pred_model = keras.Model(inputs=inputs, outputs=output)\n",
    "pred_model.summary()\n",
    "\n",
    "LEARNING_RATE=9.2e-3\n",
    "EPOCHS=85\n",
    "\n",
    "pred_model.compile(\n",
    "                loss=keras.losses.CategoricalFocalCrossentropy(alpha=0.15, gamma=0.5),\n",
    "                # optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True),\n",
    "                metrics=[\n",
    "                    keras.metrics.F1Score(average='macro'),\n",
    "                    keras.metrics.CategoricalAccuracy(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# scheduler = keras.optimizers.schedules.CosineDecay(initial_learning_rate=LEARNING_RATE, decay_steps=EPOCHS, alpha=2e-3)\n",
    "# callbacks=[\n",
    "#             keras.callbacks.EarlyStopping(patience=10, monitor='val_f1_score', mode='max', start_from_epoch=50, restore_best_weights=True),\n",
    "#             # keras.callbacks.LearningRateScheduler(schedule=scheduler),\n",
    "#         ]\n",
    "\n",
    "pred_model.fit(X_train, y_train_onehot,\n",
    "            batch_size=16,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)#,\n",
    "            # callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 2ms/step\n",
      "Training\n",
      "[[ 138  371   59]\n",
      " [  95 2800  202]\n",
      " [  29  367  279]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.24      0.33       568\n",
      "         1.0       0.79      0.90      0.84      3097\n",
      "         2.0       0.52      0.41      0.46       675\n",
      "\n",
      "    accuracy                           0.74      4340\n",
      "   macro avg       0.61      0.52      0.55      4340\n",
      "weighted avg       0.71      0.74      0.72      4340\n",
      "\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "Testing\n",
      "[[  9  60  12]\n",
      " [ 37 362  44]\n",
      " [  7  76  13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.11      0.13        81\n",
      "         1.0       0.73      0.82      0.77       443\n",
      "         2.0       0.19      0.14      0.16        96\n",
      "\n",
      "    accuracy                           0.62       620\n",
      "   macro avg       0.36      0.35      0.35       620\n",
      "weighted avg       0.57      0.62      0.59       620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_model.predict(X_train)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Training\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_train))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_train))\n",
    "\n",
    "y_pred = pred_model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandB Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pick the best possible params for the Conv-1D model, since randomly trying params wasn't fruitful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from wandb.keras import WandbCallback, WandbMetricsLogger\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2\n",
    "MOMENTUM = 0.9\n",
    "X_train_50 = None\n",
    "y_train_onehot = None\n",
    "\n",
    "def def_model():\n",
    "    inputs = keras.Input(shape=(750,))\n",
    "    x = layers.Reshape((15, 50))(inputs)\n",
    "    x = layers.SeparableConv1D(filters=32, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SeparableConv1D(filters=16, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.SeparableConv1D(filters=8, kernel_size=3, \n",
    "                            activation=keras.layers.ELU(),\n",
    "                            depthwise_initializer=keras.initializers.GlorotNormal(),\n",
    "                            pointwise_initializer=keras.initializers.GlorotNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(10, activation=keras.layers.LeakyReLU(),\n",
    "                    kernel_initializer=keras.initializers.GlorotNormal(),\n",
    "                    bias_initializer=keras.initializers.Zeros())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    global X_train_50, y_train_onehot\n",
    "    pca_x, olr_labels = load_pca_anomaly()\n",
    "    pca_x_50 = pca_x[:, :50]\n",
    "    pca_x_50 = np.array([pca_x_50[i*40+j:i*40+j+15, :] for j in range(135) for i in range(40)])\n",
    "    olr_labels = np.reshape(olr_labels, -1)\n",
    "    X_train, _, y_train, _ = train_test_split(pca_x_50, olr_labels, random_state=1337, train_size=0.875, stratify=olr_labels)\n",
    "    X_train_50 = np.reshape(X_train, (4725, -1))\n",
    "    y_train_onehot = to_categorical(y_train)\n",
    "\n",
    "\n",
    "def get_optimizer(lr=1e-3, optimizer=\"adam\"):\n",
    "    if optimizer.lower() == \"adam\":\n",
    "        return keras.optimizers.Adam(learning_rate=lr)\n",
    "    if optimizer.lower() == \"sgd\":\n",
    "        return keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    if optimizer.lower() == \"nesterov\":\n",
    "        return keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "    if optimizer.lower() == \"rmsprop\":\n",
    "        return keras.optimizers.RMSprop(earning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "def train(model, batch_size=64, epochs=10, lr=1e-3, optimizer='rmsprop', alpha=0.25, gamma=2, log_freq=10):\n",
    "    global X_train_50, y_train_onehot\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.compile(loss=keras.losses.CategoricalFocalCrossentropy(alpha=alpha, gamma=gamma), \n",
    "                  optimizer=get_optimizer(lr, optimizer), \n",
    "                  metrics=  [\n",
    "                                keras.metrics.F1Score(average='macro'),\n",
    "                                keras.metrics.CategoricalAccuracy(),\n",
    "                            ])\n",
    "    TIMESTAMP = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    mydir = os.path.join(os.getcwd(), f\"logs/logdir_{TIMESTAMP}\")\n",
    "    os.makedirs(mydir)\n",
    "    callbacks=[\n",
    "                keras.callbacks.TensorBoard(log_dir=mydir),\n",
    "                WandbCallback(log_gradients=True, training_data=(X_train_50, y_train_onehot)),\n",
    "                WandbMetricsLogger(log_freq=log_freq)\n",
    "            ]\n",
    "\n",
    "    model.fit(X_train, \n",
    "              y_train_onehot, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_split=0.1, \n",
    "              callbacks=callbacks)\n",
    "    \n",
    "\n",
    "def sweep_train(config_defaults=None):\n",
    "    with wandb.init(config=config_defaults):\n",
    "        wandb.config.architecture_name = \"Conv-1D\"\n",
    "        wandb.config.dataset_name = \"OLR\"\n",
    "\n",
    "        model = def_model()\n",
    "\n",
    "        train(model, \n",
    "              wandb.config.batch_size, \n",
    "              wandb.config.epochs,\n",
    "              wandb.config.lr,\n",
    "              wandb.config.optimizer,\n",
    "              wandb.config.alpha,\n",
    "              wandb.config.gamma)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data()\n",
    "    wandb.login()\n",
    "    sweep_configuration = {\n",
    "        \"method\": \"random\",\n",
    "        \"name\": \"sweep_rms\",\n",
    "        \"metric\":   {\n",
    "                        \"goal\": \"maximize\", \n",
    "                        \"name\": \"val_f1_score\"\n",
    "                    },  \n",
    "        \"parameters\":   {\n",
    "                            \"batch_size\":   {\n",
    "                                                \"values\": [8, 16, 32, 64, 128]\n",
    "                                            },\n",
    "                            \"epochs\":   {\n",
    "                                            \"distribution\": \"int_uniform\",\n",
    "                                            \"max\": 400,\n",
    "                                            \"min\": 15\n",
    "                                        },\n",
    "                            \"lr\":   {\n",
    "                                        \"distribution\": \"uniform\",\n",
    "                                        \"max\": 1e-2, \n",
    "                                        \"min\": 1e-6\n",
    "                                    },\n",
    "                            \"optimizer\":    {\n",
    "                                                \"values\": [\"sgd\", \"nesterov\", \"adam\", \"rmsprop\"],\n",
    "                                            },\n",
    "                            \"alpha\":{\n",
    "                                        \"values\": [0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "                                    },\n",
    "                            \"gamma\":{\n",
    "                                        \"values\": [0.5, 1, 2, 3, 4, 5]\n",
    "                                    },\n",
    "                        },\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"OLR_Base_Model\")\n",
    "    wandb.agent(sweep_id, function=sweep_train, count=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
